{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uR1JW69eLfG_"
   },
   "source": [
    "# IF3270 Pembelajaran Mesin Feedforward Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucbaI5rBLtjJ"
   },
   "source": [
    "Group Number: 18\n",
    "\n",
    "Group Members:\n",
    "- Dhafin Fawwaz Ikramullah (13522084)\n",
    "- Raden Rafly Hanggaraksa B (13522014)\n",
    "- Saad Abdul Hakim (13522092)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwzsfETHLfHA"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:20:03.767495Z",
     "start_time": "2025-03-21T07:20:02.524102Z"
    },
    "id": "jZJU5W_4LfHB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.datasets import fetch_openml\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKbjLIdYLfHC"
   },
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IWFJ-gdLfHD"
   },
   "outputs": [],
   "source": [
    "# Example of reading a csv file from a gdrive link\n",
    "\n",
    "# Take the file id from the gdrive file url\n",
    "# https://drive.google.com/file/d/1ZUtiaty9RPXhpz5F2Sy3dFPHF4YIt5iU/view?usp=sharing => The file id is 1ZUtiaty9RPXhpz5F2Sy3dFPHF4YIt5iU\n",
    "# and then put it in this format:\n",
    "# https://drive.google.com/uc?id={file_id}\n",
    "# Don't forget to change the access to public\n",
    "\n",
    "# df = pd.read_csv('https://drive.google.com/uc?id=1ZUtiaty9RPXhpz5F2Sy3dFPHF4YIt5iU')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:20:40.461243Z",
     "start_time": "2025-03-21T07:20:26.932881Z"
    }
   },
   "outputs": [],
   "source": [
    "# Authors: The scikit-learn developers\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# Turn down for faster convergence\n",
    "t0 = time.time()\n",
    "train_samples = 5000\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "\n",
    "pd.DataFrame(X).to_csv('dataset/X.csv', index=False)\n",
    "pd.DataFrame(y).to_csv('dataset/y.csv', index=False)\n",
    "\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=train_samples, test_size=10000\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Turn up tolerance for faster convergence\n",
    "clf = LogisticRegression(C=50.0 / train_samples, penalty=\"l1\", solver=\"saga\", tol=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "sparsity = np.mean(clf.coef_ == 0) * 100\n",
    "score = clf.score(X_test, y_test)\n",
    "# print('Best C % .4f' % clf.C_)\n",
    "print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
    "print(\"Test score with L1 penalty: %.4f\" % score)\n",
    "\n",
    "coef = clf.coef_.copy()\n",
    "plt.figure(figsize=(10, 5))\n",
    "scale = np.abs(coef).max()\n",
    "for k in range(10):\n",
    "    l1_plot = plt.subplot(2, 5, k + 1)\n",
    "    l1_plot.imshow(\n",
    "        coef[k].reshape(28, 28),\n",
    "        interpolation=\"nearest\",\n",
    "        cmap=plt.cm.RdBu,\n",
    "        vmin=-scale,\n",
    "        vmax=scale,\n",
    "    )\n",
    "    l1_plot.set_xticks(())\n",
    "    l1_plot.set_yticks(())\n",
    "    l1_plot.set_xlabel(\"Class %i\" % k)\n",
    "plt.suptitle(\"Classification vector for...\")\n",
    "\n",
    "run_time = time.time() - t0\n",
    "print(\"Example run in %.3f s\" % run_time)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:20:52.223060Z",
     "start_time": "2025-03-21T07:20:50.063021Z"
    }
   },
   "outputs": [],
   "source": [
    "X_csv = pd.read_csv(\"dataset/X.csv\")\n",
    "y_csv = pd.read_csv(\"dataset/y.csv\")\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, train_size=train_samples, test_size=10000\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:20:56.209946Z",
     "start_time": "2025-03-21T07:20:56.205032Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.axes._axes import Axes\n",
    "\n",
    "def visualize(X, y, row_count, col_count, offset = 0):\n",
    "    # scale = np.abs(X).max()\n",
    "    scale = 255 # in case we only pick some data and none of them reach the max value (255). \n",
    "    fig, axes = plt.subplots(row_count, col_count, figsize=(10, 10))\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "    for i in range(row_count * col_count):\n",
    "        ax: Axes = axes[i // col_count, i % col_count]\n",
    "        \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlabel(str(i+offset)+\": \"+str(int(y[i+offset])))\n",
    "        \n",
    "        ax.imshow(\n",
    "            X[i+offset].reshape(28, 28),\n",
    "            interpolation=\"nearest\",\n",
    "            cmap=plt.cm.RdBu,\n",
    "            vmin=-scale,\n",
    "            vmax=scale,\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:21:01.247382Z",
     "start_time": "2025-03-21T07:20:59.846979Z"
    }
   },
   "outputs": [],
   "source": [
    "row_count = 10\n",
    "col_count = 10\n",
    "\n",
    "X_data = X_csv.to_numpy()\n",
    "y_data_temp = y_csv.to_numpy()\n",
    "y_data = np.zeros(len(y_data_temp))\n",
    "for k in range(len(y_data_temp)):\n",
    "    y_data[k] = y_data_temp[k][0]\n",
    "\n",
    "print(X_data.shape)\n",
    "print(y_data.shape)\n",
    "\n",
    "visualize(X_data, y_data, row_count, col_count, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:22:24.835994Z",
     "start_time": "2025-03-21T07:22:17.330518Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "# X, y = make_classification(n_samples=100, random_state=1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\n",
    "clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
    "clf.predict_proba(X_test[:1])\n",
    "print(clf.score(X_test, y_test))\n",
    "# X_MLP = X_test[0:5]\n",
    "# y_MLP = clf.predict(X_MLP)\n",
    "# visualize(X_MLP, y_MLP, 1, 3)\n",
    "# print(X_MLP)\n",
    "# print(y_MLP)\n",
    "\n",
    "\n",
    "y_MLP = clf.predict(X_data)\n",
    "print(X_data.shape)\n",
    "print(y_MLP.shape)\n",
    "visualize(X_data, y_MLP, 10, 10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:22:58.755161Z",
     "start_time": "2025-03-21T07:22:58.707215Z"
    }
   },
   "outputs": [],
   "source": [
    "differences = 0\n",
    "for k in range(len(y_data)):\n",
    "    if int(y_data[k]) != int(y_MLP[k]):\n",
    "        differences += 1\n",
    "print(differences)\n",
    "\n",
    "print(str((1-differences/len(y_data)) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T08:12:12.375403Z",
     "start_time": "2025-03-21T08:12:12.349034Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Literal, Callable, Union, List\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray, ArrayLike\n",
    "from model.WeightInitialization import WeightInitiator\n",
    "\n",
    "\n",
    "class FFNNClassifier:\n",
    "    def __init__(self,\n",
    "            hidden_layer_sizes: NDArray,\n",
    "            activation_func: List[Literal['linear', 'relu', 'sigmoid', 'tanh', 'softmax']],\n",
    "            learning_rate: float,\n",
    "            verbose: int, # 0: no print, 1: print epoch progress\n",
    "            max_epoch: int,\n",
    "            batch_size: int,\n",
    "            loss_func: Literal['mean_squared_error', 'binary_cross_entropy', 'categorical_cross_entropy'],\n",
    "            init_method: Literal['normal', 'zero', 'uniform'] = 'zero',\n",
    "            lower_bound: float = 0.0,\n",
    "            upper_bound: float = 1.0,\n",
    "            mean: float = 0.0,\n",
    "            std: float = 1.0,\n",
    "            seed: int | None = None,\n",
    "        ):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.X: NDArray = []\n",
    "        self.y: list[ArrayLike] = []\n",
    "        self.weights_history: list[NDArray] = [] # array of weight matrix. index is current epoch\n",
    "        self.biases_history: list[ArrayLike] = [] # array of bias list. index is current epoch\n",
    "        self.weight_gradients_history: list[NDArray] = [] # array of weight gradients. index is current epoch\n",
    "\n",
    "        self.activation_func = activation_func\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.epoch_amount = max_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "        self.init_method = init_method\n",
    "        self.lower_bound = lower_bound\n",
    "        self.upper_bound = upper_bound\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.seed = seed\n",
    "\n",
    "\n",
    "    # return [ matrix, matrix, matrix ... ] where matrix is the weight adjacency matrix for each layer. length should be number of layers - 1 because its like the edges/connection between the nodes\n",
    "    def _generate_initial_weights(self):\n",
    "        len_features = len(self.X[0])\n",
    "        layers = np.copy([len_features])\n",
    "        len_classes = np.array([self._get_number_of_classes()])\n",
    "        layers = np.append(layers, self.hidden_layer_sizes)\n",
    "        layers = np.append(layers, len_classes)\n",
    "        print(\"layers:\", layers)\n",
    "        self.init =  WeightInitiator( \n",
    "            init_method=self.init_method,\n",
    "            nodes=layers,\n",
    "            lower_bound=self.lower_bound,\n",
    "            upper_bound=self.upper_bound,\n",
    "            mean=self.mean,\n",
    "            std=self.std,\n",
    "            seed=self.seed\n",
    "        )\n",
    "        return self.init.get_weights()\n",
    "\n",
    "    # return [ float, float, float ... ] where float is the bias for each layer. length should be number of layers - 1 because input layer does not have bias\n",
    "    def _generate_initial_biases(self):\n",
    "        bias = self.init.get_bias()\n",
    "        return bias\n",
    "\n",
    "\n",
    "# region functions\n",
    "    def _activation_function(self, x: Union[float, NDArray], func: str):\n",
    "        if func == 'linear': return x\n",
    "        elif func == 'relu': return np.maximum(0, x)\n",
    "        elif func == 'sigmoid': return 1.0/(1.0 + np.exp(-x))\n",
    "        elif func == 'tanh': return np.tanh(x)\n",
    "        elif func == 'softmax':\n",
    "            exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "            return exp_x / np.sum(exp_x, axis=1, keepdims=True) # keepdims=True will keep the dimension of the original array\n",
    "        raise \"Activation function not supported!\"\n",
    "\n",
    "    def _activation_derived_function(self, x: Union[float, NDArray], func: str):\n",
    "        if func == 'linear': return np.ones_like(x)\n",
    "        elif func == 'relu': return np.where(x > 0, 1, 0)\n",
    "        elif func == 'sigmoid':\n",
    "            sig = self._activation_function(x, 'sigmoid')\n",
    "            return sig * (1 - sig)\n",
    "        elif func == 'tanh':\n",
    "            p = 2.0/(np.exp(x) - np.exp(-x)) # should be the same as 1 - np.tanh(x) ** 2. will check later\n",
    "            return p*p  \n",
    "        elif func == 'softmax':\n",
    "            batch_size, n = x.shape\n",
    "            jacobians = np.zeros((batch_size, n, n))\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                s = x[i].reshape(-1, 1)\n",
    "                jacobians[i] = np.diagflat(s) - s @ s.T\n",
    "\n",
    "            return jacobians\n",
    "        raise \"Activation function not supported!\"\n",
    "\n",
    "\n",
    "    def _loss_function(self, y_act, y_pred, number_of_classes, func: str):\n",
    "        if func == 'mean_squared_error': return FFNNClassifier.mean_squared_error(y_act, y_pred)\n",
    "        elif func == 'binary_cross_entropy': return FFNNClassifier.binary_cross_entropy(y_act, y_pred)\n",
    "        elif func == 'categorical_cross_entropy': return FFNNClassifier.categorical_cross_entropy(y_act, y_pred)\n",
    "        \n",
    "    def mean_squared_error(y_act, y_pred):\n",
    "        return 2/len(y_act[0]) * (y_pred - y_act) # 2/len(y_act[0]) can actually be ignored because we have learning rate\n",
    "\n",
    "\n",
    "    def binary_cross_entropy(y_act, y_pred):\n",
    "        return (y_pred - y_act)/(y_pred*(1 - y_pred)) * len(y_act[0]) # len(y_act[0]) can actually be ignored because we have learning rate\n",
    "    \n",
    "    def categorical_cross_entropy(y_act, y_pred):\n",
    "        \n",
    "        return (y_pred - y_act) / len(y_act[0]) # len(y_act[0]) can actually be ignored because we have learning rate\n",
    "    \n",
    "        # -y_act / y_pred ? if activation function is not softmax\n",
    "\n",
    "# endregion functions\n",
    "\n",
    "\n",
    "# region getters setters\n",
    "\n",
    "    # Can only be called after setting X and y\n",
    "    def _get_hidden_layer_sizes(self) -> np.typing.NDArray:\n",
    "\n",
    "        len_features = len(self.X[0])\n",
    "        len_classes = self._get_number_of_classes()\n",
    "        layer_sizes = np.zeros(len(self.hidden_layer_sizes)+2, dtype=int)\n",
    "        layer_sizes[0] = len_features\n",
    "        for i in range(1, len(self.hidden_layer_sizes)+1):\n",
    "            layer_sizes[i] = self.hidden_layer_sizes[i-1]\n",
    "        layer_sizes[len(layer_sizes)-1] = len_classes\n",
    "\n",
    "        return layer_sizes\n",
    "\n",
    "    # Can only be called after setting X and y\n",
    "    def _generate_new_empty_layers(self):\n",
    "\n",
    "        layer_sizes = self._get_hidden_layer_sizes()\n",
    "        network_depth = len(layer_sizes)\n",
    "        weights = []\n",
    "        biases = []\n",
    "        nodes = []\n",
    "        nodes_active = []\n",
    "        for i in range(network_depth-1):\n",
    "            weights.append(np.zeros((layer_sizes[i], layer_sizes[i+1])))\n",
    "            biases.append(np.zeros(layer_sizes[i+1]))\n",
    "        for i in range(network_depth):\n",
    "            nodes.append(np.zeros(layer_sizes[i]))\n",
    "            nodes_active.append(np.zeros(layer_sizes[i]))\n",
    "\n",
    "        return weights, nodes, nodes_active, biases\n",
    "\n",
    "\n",
    "    # Can only be called after setting X and y\n",
    "    def _get_number_of_classes(self):\n",
    "        # return len(np.unique(self.y))\n",
    "        return 10 # hardcoded because it messes up things when the possible value is not much\n",
    "\n",
    "    def copy_list_as_zeros(self, list: list[NDArray]):\n",
    "        return [np.zeros_like(w) for w in list]\n",
    "\n",
    "# endregion getters setters\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X: NDArray, y: NDArray):\n",
    "        if type(y) == list and type(y[0]) != list:\n",
    "            y = np.array([[i] for i in y])\n",
    "        if type(y) == list and type(y[0]) == list:\n",
    "            y = np.array(y)\n",
    "\n",
    "        if len(X) != len(y):\n",
    "            raise Exception(\"length of X and y is not the same\")\n",
    "        if len(X) == 0:\n",
    "            raise Exception(\"len(self.X) == 0\")\n",
    "        if len(X[0]) == 0:\n",
    "            raise Exception(\"len(self.X[0]) == 0\")\n",
    "        if len(y) == 0:\n",
    "            raise Exception(\"len(self.y) == 0\")\n",
    "\n",
    "        # clean up in case this function is called multiple times\n",
    "        self.X: NDArray = []\n",
    "        self.y: ArrayLike = []\n",
    "        self.weights_history: list[NDArray] = []\n",
    "        self.biases_history: list[ArrayLike] = []\n",
    "        self.weight_gradients_history: list[NDArray] = []\n",
    "\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        initial_weight = self._generate_initial_weights()\n",
    "        initial_bias = self._generate_initial_biases()\n",
    "        self.weights_history = initial_weight\n",
    "        self.biases_history = initial_bias\n",
    "        initial_gradients = [np.zeros_like(w) for w in initial_weight]\n",
    "        self.weight_gradients_history.append(initial_gradients)\n",
    "\n",
    "        layer_sizes = self._get_hidden_layer_sizes()\n",
    "        network_depth = len(layer_sizes)\n",
    "        number_of_classes = self._get_number_of_classes()\n",
    "\n",
    "        for epoch in range(self.epoch_amount):\n",
    "            # for current_dataset_idx in range(len(self.X)):\n",
    "            current_dataset_idx = 0\n",
    "            while current_dataset_idx < len(self.X):\n",
    "                weights, nodes, nodes_active, biases = self._generate_new_empty_layers() # will be filled with weights based on the previous epoch. Will be appended to the history after the end of the epoch\n",
    "\n",
    "                # Feed Forward\n",
    "                until_idx = min(current_dataset_idx+self.batch_size, len(self.X))\n",
    "                nodes[0] = self.X[current_dataset_idx:until_idx]\n",
    "                nodes_active[0] = self.X[current_dataset_idx:until_idx] # not passed to activation function for the first layer\n",
    "\n",
    "                for k in range(1, network_depth):\n",
    "                    w_k = self.weights_history[k-1]\n",
    "                    b_k = self.biases_history[k-1]\n",
    "                    h_k_min_1 = nodes_active[k-1]\n",
    "\n",
    "                    a_k = b_k + np.dot(h_k_min_1, w_k) # numpy will automatically broadcast b_k (row will be copied to match the result from dot) so that this is addable\n",
    "\n",
    "                    nodes[k] = a_k\n",
    "                    nodes_active[k] = self._activation_function(a_k, self.activation_func[k-1])\n",
    "\n",
    "                # print([p.shape for p in nodes_active])\n",
    "                loss_grad = self._loss_function(\n",
    "                    y_act=self.y[current_dataset_idx:until_idx],\n",
    "                    y_pred=nodes_active[network_depth-1],\n",
    "                    number_of_classes=number_of_classes,\n",
    "                    func=self.loss_func\n",
    "                )\n",
    "                # print(\"self.y[current_dataset_idx:until_idx]: \", self.y[current_dataset_idx:until_idx])\n",
    "                # print(\"nodes_active[network_depth-1]: \", nodes_active[network_depth-1])\n",
    "                # print(\"loss_grad: \", loss_grad)\n",
    "\n",
    "\n",
    "                # Backward Propagation\n",
    "                weight_gradiens = [0 for i in range(len(self.weights_history))] # 0 will be replaced with numpy.array\n",
    "                bias_gradiens = [0 for i in range(len(self.biases_history))] # 0 will be replaced with numpy.array\n",
    "\n",
    "\n",
    "                delta = 0\n",
    "                if self.activation_func[-1] == 'softmax' and self.loss_func == 'categorical_cross_entropy':\n",
    "                    delta = loss_grad # already simplified (y_pred - y_true)\n",
    "                elif self.activation_func[-1] == 'softmax' and self.loss_func != 'categorical_cross_entropy':\n",
    "                    jacobians = self._activation_derived_function(nodes[-1], self.activation_func[-1])\n",
    "                    loss_grad_col = loss_grad[..., np.newaxis] # (batch_size, n) -> (batch_size, n, 1)\n",
    "                    delta = np.matmul(jacobians, loss_grad_col)  # (batch_size, n, 1)\n",
    "                    delta = np.squeeze(delta, axis=-1) # (batch_size, n)\n",
    "                else:\n",
    "                    delta = loss_grad * self._activation_derived_function(nodes[-1], self.activation_func[-1])\n",
    "\n",
    "\n",
    "                weight_gradiens[network_depth-2] = np.dot(nodes_active[-2].T, -delta)\n",
    "                bias_gradiens[network_depth-2] = -delta\n",
    "\n",
    "\n",
    "                for k in range(network_depth-2, 0, -1): # from the last hidden layer (not including the output layer)\n",
    "                    w = self.weights_history[k]\n",
    "\n",
    "                    delta = np.dot(delta, w.T) * self._activation_derived_function(nodes[k], self.activation_func[k-1])\n",
    "\n",
    "                    weight_gradiens[k-1] = np.dot(nodes_active[k-1].T, -delta)\n",
    "                    bias_gradiens[k-1] = -delta\n",
    "\n",
    "                self.weight_gradients_history.append(weight_gradiens)\n",
    "\n",
    "                # Update\n",
    "                for k in range(network_depth-1):\n",
    "                    w_k = self.weights_history[k]\n",
    "                    b_k = self.biases_history[k]\n",
    "\n",
    "                    weights[k] = w_k + self.learning_rate * weight_gradiens[k]\n",
    "\n",
    "                    biases[k] = b_k + self.learning_rate * bias_gradiens[k]\n",
    "                self.weights_history = weights\n",
    "                self.biases_history = biases\n",
    "\n",
    "\n",
    "                current_dataset_idx += self.batch_size\n",
    "            #### while loop ends here ##############################################\n",
    "\n",
    "\n",
    "            if self.verbose == 1:\n",
    "                print(f\"Epoch {epoch+1}/{self.epoch_amount} done\")\n",
    "            elif self.verbose == 2:\n",
    "                print(f\"========================================\")\n",
    "                print(f\"Epoch {epoch+1}/{self.epoch_amount} done\")\n",
    "                print(f\"weights: {self.weights_history}\")\n",
    "                print(f\"biases: {self.biases_history}\")\n",
    "\n",
    "\n",
    "    def predict(self, X_test: NDArray):\n",
    "        prediction = [-1 for i in range(len(X_test))]\n",
    "        current_idx = 0\n",
    "        while current_idx < len(X_test):\n",
    "            weights, nodes, nodes_active, biases = self._generate_new_empty_layers()\n",
    "            until_idx = min(current_idx+self.batch_size, len(X_test))\n",
    "            nodes[0] = X_test[current_idx:until_idx]\n",
    "            nodes_active[0] = X_test[current_idx:until_idx]\n",
    "\n",
    "            for k in range(1, len(self.weights_history)+1):\n",
    "                w_k = self.weights_history[k-1]\n",
    "                b_k = self.biases_history[k-1]\n",
    "                h_k_min_1 = nodes_active[k-1]\n",
    "\n",
    "                a_k = b_k + np.dot(h_k_min_1, w_k)\n",
    "\n",
    "                nodes[k] = a_k\n",
    "                nodes_active[k] = self._activation_function(a_k, self.activation_func[k-1])\n",
    "            predicted_class = [int(np.argmax(nodes_active[-1][i])) for i in range(len(nodes_active[-1]))] # idx with highest value. idx is also the class\n",
    "            prediction[current_idx:until_idx] = predicted_class\n",
    "            current_idx += self.batch_size\n",
    "\n",
    "        return prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:26:36.986873Z",
     "start_time": "2025-03-21T07:26:36.979972Z"
    }
   },
   "outputs": [],
   "source": [
    "# Case edunex\n",
    "\n",
    "ffnn = FFNNClassifier(\n",
    "    hidden_layer_sizes=[2],\n",
    "    activation_func=\"sigmoid\",\n",
    "    learning_rate=0.5,\n",
    "    verbose=1,\n",
    "    max_epoch=1,\n",
    "    batch_size=1,\n",
    "    loss_func=\"mean_squared_error\",\n",
    "    init_method=\"normal\",\n",
    "    lower_bound=4,\n",
    "    upper_bound=7,\n",
    "    mean=24,\n",
    "    std=2,\n",
    "    seed=69\n",
    ")\n",
    "def _generate_initial_weights():\n",
    "    return [np.array([[0.15, 0.25], [0.2, 0.3]]), np.array([[0.4, 0.5], [0.45, 0.55]])]\n",
    "def _generate_initial_biases():\n",
    "    return [np.array([0.35, 0.35]), np.array([0.6, 0.6])]\n",
    "# ffnn._generate_initial_weights = _generate_initial_weights\n",
    "# ffnn._generate_initial_biases = _generate_initial_biases\n",
    "\n",
    "X_temp = np.array([[0.05, 0.1]])\n",
    "y_temp = np.array([[0.01, 0.99]])\n",
    "ffnn.fit(X_temp, y_temp)\n",
    "# prediction = ffnn.predict(X_temp)\n",
    "# print(prediction)\n",
    "\n",
    "print(\"Final Weights:\", ffnn.weights_history)\n",
    "print(\"Final Biases:\", ffnn.biases_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T07:26:35.237527Z",
     "start_time": "2025-03-21T07:26:35.229565Z"
    }
   },
   "outputs": [],
   "source": [
    "# Case random\n",
    "\n",
    "ffnn = FFNNClassifier(\n",
    "    hidden_layer_sizes=[3,2,5],\n",
    "    activation_func=\"sigmoid\",\n",
    "    learning_rate=0.5,\n",
    "    verbose=1,\n",
    "    max_epoch=5,\n",
    "    batch_size=2,\n",
    "    loss_func=\"mean_squared_error\"\n",
    ")\n",
    "\n",
    "X_temp = np.array([[0.05, 0.1, 0.2, 0.25], [0.05, 0.1, 0.2, 0.25], [0.05, 0.1, 0.2, 0.25], [0.05, 0.1, 0.2, 0.25]])\n",
    "y_temp = np.array([[0.01, 0.99], [0.01, 0.99], [0.01, 0.99], [0.01, 0.99]])\n",
    "ffnn.fit(X_temp, y_temp)\n",
    "prediction = ffnn.predict(X_temp)\n",
    "print(\"Prediction:\", prediction)\n",
    "print(\"Final Weights:\", ffnn.weights_history)\n",
    "print(\"Final Biases:\", ffnn.biases_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T08:26:12.497248Z",
     "start_time": "2025-03-21T08:25:51.963909Z"
    }
   },
   "outputs": [],
   "source": [
    "# Case random\n",
    "def one_hot_encode(y):\n",
    "    num_of_classes = 10\n",
    "    arr = []\n",
    "    for i in range(len(y)):\n",
    "        arr.append([0 for j in range(num_of_classes)]) # hardcoded 10 for the number of classes\n",
    "        arr[i][y[i]] = 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "ffnn = FFNNClassifier(\n",
    "    hidden_layer_sizes=[256, 128, 64],\n",
    "    activation_func=[\"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\"],\n",
    "    learning_rate=0.05,\n",
    "    verbose=1,\n",
    "    max_epoch=15,\n",
    "    batch_size=50,\n",
    "    loss_func=\"mean_squared_error\",\n",
    "    init_method=\"normal\",\n",
    "    lower_bound=5.39294405e-05,\n",
    "    upper_bound=1,\n",
    "    mean=5.39294405e-05,\n",
    "    std=.44,\n",
    "    seed=69\n",
    ")\n",
    "\n",
    "train_until_idx = 10000\n",
    "test_until_idx = 100\n",
    "x_sliced = X_data[0:train_until_idx]\n",
    "y_sliced = y_data[0:train_until_idx]\n",
    "y_sliced = [int(i) for i in y_sliced]\n",
    "y_one_hot = one_hot_encode([int(i) for i in y_sliced])\n",
    "\n",
    "# print(x_sliced)\n",
    "# print(y_sliced)\n",
    "print(len(np.unique(y_one_hot)))\n",
    "ffnn.fit(x_sliced, y_one_hot)\n",
    "prediction = ffnn.predict(X_data[0:test_until_idx])\n",
    "# print(\"Prediction:\", prediction)\n",
    "# print(\"Expected:\", y_sliced[0:test_until_idx])\n",
    "# print(\"Final Weights:\", ffnn.weights_history)\n",
    "# print(\"Final Biases:\", ffnn.biases_history[-1])\n",
    "\n",
    "def get_same_count(y1, y2):\n",
    "    count = 0\n",
    "    for i in range(len(y1)):\n",
    "        if y1[i] == y2[i]:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "print(\"Accuracy:\", get_same_count(prediction, y_sliced)/len(prediction) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arrtest = np.array([[0.5 for j in range(512)] for i in range(784)])\n",
    "print(arrtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "ffnn = MLPClassifier(\n",
    "    hidden_layer_sizes=(3),\n",
    "    activation=\"relu\",\n",
    "    learning_rate=\"constant\",\n",
    "    learning_rate_init=0.5,\n",
    "    verbose=0,\n",
    "    max_iter=500,\n",
    "    batch_size=2,\n",
    "    # loss_func=\"mean_squared_error\"\n",
    ")\n",
    "y_target = [0,1,2,3]\n",
    "print(y_target)\n",
    "ffnn.fit(X_temp, y_target)\n",
    "# prediction = ffnn.predict(X_temp)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
