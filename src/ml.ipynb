{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR1JW69eLfG_"
      },
      "source": [
        "# IF3270 Pembelajaran Mesin Feedforward Neural Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucbaI5rBLtjJ"
      },
      "source": [
        "Group Number: 18\n",
        "\n",
        "Group Members:\n",
        "- Dhafin Fawwaz Ikramullah (13522084)\n",
        "- Raden Rafly Hanggaraksa B (13522014)\n",
        "- Saad Abdul Hakim (13522092)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzsfETHLfHA"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "jZJU5W_4LfHB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.datasets import fetch_openml\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import check_random_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKbjLIdYLfHC"
      },
      "source": [
        "## Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IWFJ-gdLfHD"
      },
      "outputs": [],
      "source": [
        "# Example of reading a csv file from a gdrive link\n",
        "\n",
        "# Take the file id from the gdrive file url\n",
        "# https://drive.google.com/file/d/1ZUtiaty9RPXhpz5F2Sy3dFPHF4YIt5iU/view?usp=sharing => The file id is 1ZUtiaty9RPXhpz5F2Sy3dFPHF4YIt5iU\n",
        "# and then put it in this format:\n",
        "# https://drive.google.com/uc?id={file_id}\n",
        "# Don't forget to change the access to public\n",
        "\n",
        "# df = pd.read_csv('https://drive.google.com/uc?id=1ZUtiaty9RPXhpz5F2Sy3dFPHF4YIt5iU')\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authors: The scikit-learn developers\n",
        "# SPDX-License-Identifier: BSD-3-Clause\n",
        "\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "# Turn down for faster convergence\n",
        "t0 = time.time()\n",
        "train_samples = 5000\n",
        "\n",
        "# Load data from https://www.openml.org/d/554\n",
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
        "\n",
        "pd.DataFrame(X).to_csv('dataset/X.csv', index=False)\n",
        "pd.DataFrame(y).to_csv('dataset/y.csv', index=False)\n",
        "\n",
        "random_state = check_random_state(0)\n",
        "permutation = random_state.permutation(X.shape[0])\n",
        "X = X[permutation]\n",
        "y = y[permutation]\n",
        "X = X.reshape((X.shape[0], -1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, train_size=train_samples, test_size=10000\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Turn up tolerance for faster convergence\n",
        "# clf = LogisticRegression(C=50.0 / train_samples, penalty=\"l1\", solver=\"saga\", tol=0.1)\n",
        "# clf.fit(X_train, y_train)\n",
        "# sparsity = np.mean(clf.coef_ == 0) * 100\n",
        "# score = clf.score(X_test, y_test)\n",
        "# # print('Best C % .4f' % clf.C_)\n",
        "# print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
        "# print(\"Test score with L1 penalty: %.4f\" % score)\n",
        "\n",
        "# coef = clf.coef_.copy()\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# scale = np.abs(coef).max()\n",
        "# for k in range(10):\n",
        "#     l1_plot = plt.subplot(2, 5, k + 1)\n",
        "#     l1_plot.imshow(\n",
        "#         coef[k].reshape(28, 28),\n",
        "#         interpolation=\"nearest\",\n",
        "#         cmap=plt.cm.RdBu,\n",
        "#         vmin=-scale,\n",
        "#         vmax=scale,\n",
        "#     )\n",
        "#     l1_plot.set_xticks(())\n",
        "#     l1_plot.set_yticks(())\n",
        "#     l1_plot.set_xlabel(\"Class %i\" % k)\n",
        "# plt.suptitle(\"Classification vector for...\")\n",
        "\n",
        "# run_time = time.time() - t0\n",
        "# print(\"Example run in %.3f s\" % run_time)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_csv = pd.read_csv(\"dataset/X.csv\")\n",
        "y_csv = pd.read_csv(\"dataset/y.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib.axes._axes import Axes\n",
        "\n",
        "def visualize(X, y, row_count, col_count, offset = 0):\n",
        "    # scale = np.abs(X).max()\n",
        "    scale = 255 # in case we only pick some data and none of them reach the max value (255). \n",
        "    fig, axes = plt.subplots(row_count, col_count, figsize=(10, 10))\n",
        "    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
        "\n",
        "    for i in range(row_count * col_count):\n",
        "        ax: Axes = axes[i // col_count, i % col_count]\n",
        "        \n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_xlabel(str(i+offset)+\": \"+str(int(y[i+offset])))\n",
        "        \n",
        "        ax.imshow(\n",
        "            X[i+offset].reshape(28, 28),\n",
        "            interpolation=\"nearest\",\n",
        "            cmap=plt.cm.RdBu,\n",
        "            vmin=-scale,\n",
        "            vmax=scale,\n",
        "        )\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "row_count = 10\n",
        "col_count = 10\n",
        "\n",
        "X_data = X_csv.to_numpy()\n",
        "X_data = X_data[:, 1:]\n",
        "y_data_temp = y_csv.to_numpy()\n",
        "y_data = np.zeros(len(y_data_temp))\n",
        "for k in range(len(y_data_temp)):\n",
        "    y_data[k] = y_data_temp[k][1]\n",
        "\n",
        "print(X_data.shape)\n",
        "print(y_data.shape)\n",
        "\n",
        "visualize(X_data, y_data, row_count, col_count, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FFNN:\n",
        "    def __init__(self):\n",
        "        self.x = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "a_k = FFNN()\n",
        "print(a_k.x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## With Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "# X, y = make_classification(n_samples=100, random_state=1)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\n",
        "clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
        "clf.predict_proba(X_test[:1])\n",
        "print(clf.score(X_test, y_test))\n",
        "# X_MLP = X_test[0:5]\n",
        "# y_MLP = clf.predict(X_MLP)\n",
        "# visualize(X_MLP, y_MLP, 1, 3)\n",
        "# print(X_MLP)\n",
        "# print(y_MLP)\n",
        "\n",
        "\n",
        "y_MLP = clf.predict(X_data)\n",
        "print(X_data.shape)\n",
        "print(y_MLP.shape)\n",
        "visualize(X_data, y_MLP, 10, 10, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "differences = 0\n",
        "for k in range(len(y_data)):\n",
        "    if int(y_data[k]) != int(y_MLP[k]):\n",
        "        differences += 1\n",
        "print(differences)\n",
        "\n",
        "print(str((1-differences/len(y_data)) * 100) + \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Literal, Callable, Union\n",
        "import numpy as np\n",
        "from numpy.typing import NDArray, ArrayLike\n",
        "\n",
        "\n",
        "class FFNNClassifier:\n",
        "    def __init__(self, \n",
        "            hidden_layer_sizes: NDArray,\n",
        "            activation_func: Literal['linear', 'relu', 'sigmoid', 'tanh', 'softmax'], \n",
        "            learning_rate: float,\n",
        "            verbose: int, # 0: no print, 1: print epoch progress\n",
        "            max_epoch: int,\n",
        "            batch_size: int,\n",
        "            loss_func: Literal['mean_squared_error', 'binary_cross_entropy', 'categorical_cross_entropy']\n",
        "        ):\n",
        "        self.hidden_layer_sizes = hidden_layer_sizes\n",
        "        self.X: NDArray = []\n",
        "        self.y: list[ArrayLike] = []\n",
        "        self.weights_history: list[NDArray] = [] # array of weight matrix. index is current epoch\n",
        "        self.biases_history: list[ArrayLike] = [] # array of bias list. index is current epoch\n",
        "        self.weight_gradients_history: list[NDArray] = [] # array of weight gradients. index is current epoch\n",
        "\n",
        "        self.activation_func = activation_func\n",
        "        self.learning_rate = learning_rate\n",
        "        self.verbose = verbose\n",
        "        self.epoch_amount = max_epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.loss_func = loss_func\n",
        "\n",
        "    # return [ matrix, matrix, matrix ... ] where matrix is the weight adjacency matrix for each layer. length should be number of layers - 1 because its like the edges/connection between the nodes\n",
        "    def _generate_initial_weights(self):\n",
        "        \n",
        "        # delete this\n",
        "        weights, nodes, nodes_active, biases = self._generate_new_empty_layers()\n",
        "        return weights\n",
        "        # return [np.array([[0.15, 0.25], [0.2, 0.3]]), np.array([[0.4, 0.5], [0.45, 0.55]])]\n",
        "    \n",
        "    # return [ float, float, float ... ] where float is the bias for each layer. length should be number of layers - 1 because input layer does not have bias\n",
        "    def _generate_initial_biases(self):\n",
        "        \n",
        "        # delete this\n",
        "        weights, nodes, nodes_active, biases = self._generate_new_empty_layers()\n",
        "        return biases\n",
        "        # return [np.array([0.35, 0.35]), np.array([0.6, 0.6])]\n",
        "        \n",
        "\n",
        "# region functions\n",
        "    def _activation_function(self, x: Union[float, NDArray], func: str):\n",
        "        if func == 'linear': return x\n",
        "        elif func == 'relu': return np.maximum(0, x)\n",
        "        elif func == 'sigmoid': return 1.0/(1.0 + np.exp(-x))\n",
        "        elif func == 'tanh': return np.tanh(x)\n",
        "        elif func == 'softmax':\n",
        "            exp_x = np.exp(x - np.max(x, axis=1, keepdims=True)) \n",
        "            return exp_x / np.sum(exp_x, axis=1, keepdims=True) # keepdims=True will keep the dimension of the original array\n",
        "        raise \"Activation function not supported!\"\n",
        "    \n",
        "    def _activation_derived_function(self, x: Union[float, NDArray], func: str):\n",
        "        if func == 'linear': return np.ones_like(x)\n",
        "        elif func == 'relu': return np.where(x > 0, 1, 0)\n",
        "        elif func == 'sigmoid':\n",
        "            sig = self._activation_function(x, 'sigmoid')\n",
        "            return sig * (1 - sig)\n",
        "        elif func == 'tanh': \n",
        "            p = 2.0/(np.exp(x) - np.exp(-x)) # should be the same as 1 - np.tanh(x) ** 2. will check later\n",
        "            return p*p\n",
        "        elif func == 'softmax': # TODO: Check if this implementation is correct\n",
        "            delta = np.zeros_like(x) \n",
        "            for i in range(x.shape[0]): delta[i, i] = 1\n",
        "            softmax = self._activation_function(x, 'softmax')\n",
        "            return softmax*(delta-softmax) \n",
        "            # s = self._activation_function(x, 'softmax')\n",
        "            # return s * (1 - s)\n",
        "        raise \"Activation function not supported!\"\n",
        "    \n",
        "\n",
        "    def _loss_function(self, y_act, y_pred, number_of_classes, func: str):\n",
        "        if func == 'mean_squared_error': return FFNNClassifier.mean_squared_error(y_act, y_pred)\n",
        "        elif func == 'binary_cross_entropy': return FFNNClassifier.binary_cross_entropy(y_act, y_pred)\n",
        "        elif func == 'categorical_cross_entropy': return FFNNClassifier.categorical_cross_entropy(y_act, y_pred, number_of_classes)\n",
        "    def mean_squared_error(y_act, y_pred):\n",
        "        return 2/len(y_act[0]) * (y_pred - y_act)\n",
        "    def binary_cross_entropy(y_act, y_pred, n):\n",
        "        return (y_pred - y_act)/(y_pred*(1 - y_pred))\n",
        "    def categorical_cross_entropy(y_act, y_pred, c):\n",
        "        return y_pred - y_act\n",
        "\n",
        "# endregion functions\n",
        "\n",
        "\n",
        "# region getters setters\n",
        "    \n",
        "    # Can only be called after setting X and y\n",
        "    def _get_hidden_layer_sizes(self) -> np.typing.NDArray:\n",
        "        \n",
        "        len_features = len(self.X[0])\n",
        "        len_classes = self._get_number_of_classes()\n",
        "        layer_sizes = np.zeros(len(self.hidden_layer_sizes)+2, dtype=int)\n",
        "        layer_sizes[0] = len_features\n",
        "        for i in range(1, len(self.hidden_layer_sizes)+1):\n",
        "            layer_sizes[i] = self.hidden_layer_sizes[i-1]\n",
        "        layer_sizes[len(layer_sizes)-1] = len_classes\n",
        "        \n",
        "        return layer_sizes\n",
        "    \n",
        "    # Can only be called after setting X and y\n",
        "    def _generate_new_empty_layers(self):\n",
        "        \n",
        "        layer_sizes = self._get_hidden_layer_sizes()\n",
        "        network_depth = len(layer_sizes)\n",
        "        weights = []\n",
        "        biases = []\n",
        "        nodes = []\n",
        "        nodes_active = []\n",
        "        for i in range(network_depth-1):\n",
        "            weights.append(np.zeros((layer_sizes[i], layer_sizes[i+1])))\n",
        "            biases.append(np.zeros(layer_sizes[i+1]))\n",
        "        for i in range(network_depth):\n",
        "            nodes.append(np.zeros(layer_sizes[i]))\n",
        "            nodes_active.append(np.zeros(layer_sizes[i]))\n",
        "        \n",
        "        return weights, nodes, nodes_active, biases\n",
        "\n",
        "\n",
        "    # Can only be called after setting X and y\n",
        "    def _get_number_of_classes(self):\n",
        "        return len(np.unique(self.y))\n",
        "    \n",
        "    def copy_list_as_zeros(self, list: list[NDArray]):\n",
        "        return [np.zeros_like(w) for w in list]\n",
        "    \n",
        "# endregion getters setters\n",
        "\n",
        "    def fit(self, X: NDArray, y: NDArray):\n",
        "        if len(X) != len(y):\n",
        "            raise Exception(\"length of X and y is not the same\")\n",
        "        if len(X) == 0:\n",
        "            raise Exception(\"len(self.X) == 0\")\n",
        "        if len(X[0]) == 0:\n",
        "            raise Exception(\"len(self.X[0]) == 0\")\n",
        "        if len(y) == 0:\n",
        "            raise Exception(\"len(self.y) == 0\")\n",
        "        \n",
        "        # clean up in case this function is called multiple times\n",
        "        self.X: NDArray = []\n",
        "        self.y: ArrayLike = []\n",
        "        self.weights_history: list[NDArray] = []\n",
        "        self.biases_history: list[ArrayLike] = []\n",
        "        self.weight_gradients_history: list[NDArray] = []\n",
        "\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        initial_weight = self._generate_initial_weights()\n",
        "        initial_bias = self._generate_initial_biases()\n",
        "        self.weights_history.append(initial_weight)\n",
        "        self.biases_history.append(initial_bias)\n",
        "        initial_gradients = [np.zeros_like(w) for w in initial_weight]\n",
        "        self.weight_gradients_history.append(initial_gradients)\n",
        "\n",
        "        layer_sizes = self._get_hidden_layer_sizes()\n",
        "        network_depth = len(layer_sizes)\n",
        "        number_of_classes = self._get_number_of_classes()\n",
        "\n",
        "        for epoch in range(self.epoch_amount):\n",
        "            # for current_dataset_idx in range(len(self.X)):\n",
        "            current_dataset_idx = 0\n",
        "            while current_dataset_idx < len(self.X):\n",
        "                weights, nodes, nodes_active, biases = self._generate_new_empty_layers() # will be filled with weights based on the previous epoch. Will be appended to the history after the end of the epoch\n",
        "\n",
        "                # Feed Forward\n",
        "                until_idx = min(current_dataset_idx+self.batch_size, len(self.X))\n",
        "                nodes[0] = self.X[current_dataset_idx:until_idx]\n",
        "                nodes_active[0] = self.X[current_dataset_idx:until_idx] # not passed to activation function for the first layer\n",
        "                \n",
        "                for k in range(1, network_depth):\n",
        "                    w_k = self.weights_history[-1][k-1]\n",
        "                    b_k = self.biases_history[-1][k-1]\n",
        "                    h_k_min_1 = nodes_active[k-1]\n",
        "\n",
        "                    a_k = b_k + np.dot(h_k_min_1, w_k) # numpy will automatically broadcast b_k (row will be copied to match the result from dot) so that this is addable\n",
        "\n",
        "                    nodes[k] = a_k\n",
        "                    nodes_active[k] = self._activation_function(a_k, self.activation_func)\n",
        "                \n",
        "                loss_grad = self._loss_function(\n",
        "                    y_act=self.y[current_dataset_idx:until_idx], \n",
        "                    y_pred=nodes_active[network_depth-1], \n",
        "                    number_of_classes=number_of_classes, \n",
        "                    func=self.loss_func\n",
        "                )\n",
        "\n",
        "\n",
        "                # Backward Propagation\n",
        "                weight_gradiens = [0 for i in range(len(self.weights_history[-1]))] # 0 will be replaced with numpy.array\n",
        "                bias_gradiens = [0 for i in range(len(self.biases_history[-1]))] # 0 will be replaced with numpy.array\n",
        "                \n",
        "                delta = loss_grad * self._activation_derived_function(nodes[-1], self.activation_func)\n",
        "\n",
        "                weight_gradiens[network_depth-2] = np.dot(nodes_active[-2].T, -delta)\n",
        "                bias_gradiens[network_depth-2] = -delta\n",
        "                \n",
        "                \n",
        "                for k in range(network_depth-2, 0, -1): # from the last hidden layer (not including the output layer)\n",
        "                    w = self.weights_history[-1][k]\n",
        "\n",
        "                    delta = np.dot(delta, w.T) * self._activation_derived_function(nodes[k], self.activation_func)\n",
        "\n",
        "                    weight_gradiens[k-1] = np.dot(nodes_active[k-1].T, -delta)\n",
        "                    bias_gradiens[k-1] = -delta\n",
        "                \n",
        "                self.weight_gradients_history.append(weight_gradiens)\n",
        "                \n",
        "                # Update\n",
        "                for k in range(network_depth-1):\n",
        "                    w_k = self.weights_history[-1][k]\n",
        "                    b_k = self.biases_history[-1][k]\n",
        "\n",
        "                    weights[k] = w_k + self.learning_rate * weight_gradiens[k]\n",
        "\n",
        "                    biases[k] = b_k + self.learning_rate * bias_gradiens[k]\n",
        "                self.weights_history.append(weights)\n",
        "                self.biases_history.append(biases)\n",
        "\n",
        "            \n",
        "                current_dataset_idx += self.batch_size\n",
        "            #### while loop ends here ##############################################\n",
        "\n",
        "\n",
        "            if self.verbose == 1:\n",
        "                print(f\"Epoch {epoch+1}/{self.epoch_amount} done\")\n",
        "            elif self.verbose == 2:\n",
        "                print(f\"========================================\")\n",
        "                print(f\"Epoch {epoch+1}/{self.epoch_amount} done\")\n",
        "                print(f\"weights: {self.weights_history[-1]}\")\n",
        "                print(f\"biases: {self.biases_history[-1]}\")\n",
        "\n",
        "\n",
        "    def predict(self, X_test: NDArray):\n",
        "        prediction = [-1 for i in range(len(X_test))]\n",
        "        current_idx = 0\n",
        "        while current_idx < len(X_test):\n",
        "            weights, nodes, nodes_active, biases = self._generate_new_empty_layers()\n",
        "            until_idx = min(current_idx+self.batch_size, len(X_test))\n",
        "            nodes[0] = X_test[current_idx:until_idx]\n",
        "            nodes_active[0] = X_test[current_idx:until_idx]\n",
        "\n",
        "            for k in range(1, len(self.weights_history[-1])+1):\n",
        "                w_k = self.weights_history[-1][k-1]\n",
        "                b_k = self.biases_history[-1][k-1]\n",
        "                h_k_min_1 = nodes_active[k-1]\n",
        "\n",
        "                a_k = b_k + np.dot(h_k_min_1, w_k)\n",
        "\n",
        "                nodes[k] = a_k\n",
        "                nodes_active[k] = self._activation_function(a_k, self.activation_func)\n",
        "            \n",
        "            predicted_class = [int(np.argmax(nodes_active[-1][i])) for i in range(len(nodes_active[-1]))] # idx with highest value. idx is also the class\n",
        "            prediction[current_idx:until_idx] = predicted_class\n",
        "            current_idx += self.batch_size\n",
        "            \n",
        "        return prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1 done\n",
            "Final Weights: [array([[0.14978072, 0.24975114],\n",
            "       [0.19956143, 0.29950229]]), array([[0.35891648, 0.51130127],\n",
            "       [0.40866619, 0.56137012]])]\n",
            "Final Biases: [array([[0.34561432, 0.34502287]]), array([[0.53075072, 0.61904912]])]\n"
          ]
        }
      ],
      "source": [
        "# Case edunex\n",
        "\n",
        "ffnn = FFNNClassifier(\n",
        "    hidden_layer_sizes=[3],\n",
        "    activation_func=\"sigmoid\",\n",
        "    learning_rate=0.5,\n",
        "    verbose=1,\n",
        "    max_epoch=1,\n",
        "    batch_size=1,\n",
        "    loss_func=\"mean_squared_error\"\n",
        ")\n",
        "def _generate_initial_weights():\n",
        "    return [np.array([[0.15, 0.25], [0.2, 0.3]]), np.array([[0.4, 0.5], [0.45, 0.55]])]\n",
        "def _generate_initial_biases():\n",
        "    return [np.array([0.35, 0.35]), np.array([0.6, 0.6])]\n",
        "ffnn._generate_initial_weights = _generate_initial_weights\n",
        "ffnn._generate_initial_biases = _generate_initial_biases\n",
        "\n",
        "X_temp = np.array([[0.05, 0.1]])\n",
        "y_temp = np.array([[0.01, 0.99]])\n",
        "ffnn.fit(X_temp, y_temp)\n",
        "# prediction = ffnn.predict(X_temp)\n",
        "# print(prediction)\n",
        "\n",
        "print(\"Final Weights:\", ffnn.weights_history[-1])\n",
        "print(\"Final Biases:\", ffnn.biases_history[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 done\n",
            "Epoch 2/5 done\n",
            "Epoch 3/5 done\n",
            "Epoch 4/5 done\n",
            "Epoch 5/5 done\n",
            "Prediction: [1, 1, 1, 1]\n",
            "Final Weights: [array([[4.34367662e-09, 4.34367662e-09, 4.34367662e-09],\n",
            "       [8.68735324e-09, 8.68735324e-09, 8.68735324e-09],\n",
            "       [1.73747065e-08, 1.73747065e-08, 1.73747065e-08],\n",
            "       [2.17183831e-08, 2.17183831e-08, 2.17183831e-08]]), array([[0.00045716, 0.00045716],\n",
            "       [0.00045716, 0.00045716],\n",
            "       [0.00045716, 0.00045716]]), array([[0.02875542, 0.02875542, 0.02875542, 0.02875542, 0.02875542],\n",
            "       [0.02875542, 0.02875542, 0.02875542, 0.02875542, 0.02875542]]), array([[-0.36255151,  0.36255151],\n",
            "       [-0.36255151,  0.36255151],\n",
            "       [-0.36255151,  0.36255151],\n",
            "       [-0.36255151,  0.36255151],\n",
            "       [-0.36255151,  0.36255151]])]\n",
            "Final Biases: [array([[4.34367662e-08, 4.34367662e-08, 4.34367662e-08],\n",
            "       [4.34367662e-08, 4.34367662e-08, 4.34367662e-08]]), array([[0.00045716, 0.00045716],\n",
            "       [0.00045716, 0.00045716]]), array([[0.0287512, 0.0287512, 0.0287512, 0.0287512, 0.0287512],\n",
            "       [0.0287512, 0.0287512, 0.0287512, 0.0287512, 0.0287512]]), array([[-0.35988306,  0.35988306],\n",
            "       [-0.35988306,  0.35988306]])]\n"
          ]
        }
      ],
      "source": [
        "# Case random\n",
        "\n",
        "ffnn = FFNNClassifier(\n",
        "    hidden_layer_sizes=[3,2,5],\n",
        "    activation_func=\"sigmoid\",\n",
        "    learning_rate=0.5,\n",
        "    verbose=1,\n",
        "    max_epoch=5,\n",
        "    batch_size=2,\n",
        "    loss_func=\"mean_squared_error\"\n",
        ")\n",
        "\n",
        "X_temp = np.array([[0.05, 0.1, 0.2, 0.25], [0.05, 0.1, 0.2, 0.25], [0.05, 0.1, 0.2, 0.25], [0.05, 0.1, 0.2, 0.25]])\n",
        "y_temp = np.array([[0.01, 0.99], [0.01, 0.99], [0.01, 0.99], [0.01, 0.99]])\n",
        "ffnn.fit(X_temp, y_temp)\n",
        "prediction = ffnn.predict(X_temp)\n",
        "print(\"Prediction:\", prediction)\n",
        "print(\"Final Weights:\", ffnn.weights_history[-1])\n",
        "print(\"Final Biases:\", ffnn.biases_history[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 1, 2, 3]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-4 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-4 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-4 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-4 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-4 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-4 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-4 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-4 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-4 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-4 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=2, hidden_layer_sizes=3, learning_rate_init=0.5,\n",
              "              max_iter=500, verbose=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(batch_size=2, hidden_layer_sizes=3, learning_rate_init=0.5,\n",
              "              max_iter=500, verbose=0)</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "MLPClassifier(batch_size=2, hidden_layer_sizes=3, learning_rate_init=0.5,\n",
              "              max_iter=500, verbose=0)"
            ]
          },
          "execution_count": 296,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "ffnn = MLPClassifier(\n",
        "    hidden_layer_sizes=(3),\n",
        "    activation=\"relu\",\n",
        "    learning_rate=\"constant\",\n",
        "    learning_rate_init=0.5,\n",
        "    verbose=0,\n",
        "    max_iter=500,\n",
        "    batch_size=2,\n",
        "    # loss_func=\"mean_squared_error\"\n",
        ")\n",
        "y_target = [0,1,2,3]\n",
        "print(y_target)\n",
        "ffnn.fit(X_temp, y_target)\n",
        "# prediction = ffnn.predict(X_temp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from scipy import stats\n",
        "\n",
        "class NeuralNetworkVisualizer:\n",
        "    def __init__(self, layers, weights=None, gradients=None, biases=None):\n",
        "        self.layers = layers\n",
        "\n",
        "        if weights is not None:\n",
        "            self.weights = [w[0] if isinstance(w, list) and isinstance(w[0], np.ndarray) else w for w in weights]\n",
        "        else:\n",
        "            self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers) - 1)]\n",
        "\n",
        "        if gradients is not None:\n",
        "            self.gradients = [g[0] if isinstance(g, list) and isinstance(g[0], np.ndarray) else g for g in gradients]\n",
        "        else:\n",
        "            self.gradients = [np.random.randn(*w.shape) * 0.1 for w in self.weights]\n",
        "            \n",
        "        if biases is not None:\n",
        "            self.biases = [b[0] if isinstance(b, list) and isinstance(b[0], np.ndarray) else b for b in biases]\n",
        "        else:\n",
        "            self.biases = [np.random.randn(layers[i+1]) for i in range(len(layers) - 1)]\n",
        "\n",
        "    def plot_network(self):\n",
        "        G = nx.DiGraph()\n",
        "        positions = {}\n",
        "        node_count = 0\n",
        "        layer_spacing = 3\n",
        "        node_spacing = 1\n",
        "        colors = []\n",
        "        node_labels = {}\n",
        "\n",
        "        for layer_idx, num_nodes in enumerate(self.layers):\n",
        "            layer_start_node = node_count\n",
        "            for node_idx in range(num_nodes):\n",
        "                node_id = node_count\n",
        "                positions[node_id] = (layer_idx * layer_spacing, -node_idx * node_spacing)\n",
        "                G.add_node(node_id)\n",
        "                node_labels[node_id] = f\"{layer_idx},{node_idx}\"\n",
        "                if layer_idx == 0:\n",
        "                    colors.append(\"red\")  # Input layer\n",
        "                elif layer_idx == len(self.layers) - 1:\n",
        "                    colors.append(\"green\")  # Output layer\n",
        "                else:\n",
        "                    colors.append(\"blue\")  # Hidden layer\n",
        "                node_count += 1\n",
        "\n",
        "        edge_weights = {}\n",
        "        node_count = 0\n",
        "        \n",
        "        for layer_idx in range(len(self.layers) - 1):\n",
        "            layer_size = self.layers[layer_idx]\n",
        "            next_layer_size = self.layers[layer_idx + 1]\n",
        "            next_layer_start = node_count + layer_size\n",
        "            \n",
        "            for src in range(node_count, node_count + layer_size):\n",
        "                for dst_idx, dst in enumerate(range(next_layer_start, next_layer_start + next_layer_size)):\n",
        "                    weight = self.weights[layer_idx][src - node_count, dst_idx]\n",
        "                    G.add_edge(src, dst, weight=weight)\n",
        "                    edge_weights[(src, dst)] = f'{weight:.2f}'\n",
        "            \n",
        "            node_count += layer_size\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        \n",
        "        nx.draw(G, pos=positions, with_labels=False, node_size=500, node_color=colors, edge_color=\"gray\")\n",
        "        \n",
        "        edge_labels_pos = {}\n",
        "        for (u, v), label in edge_weights.items():\n",
        "            u_x, u_y = positions[u]\n",
        "            v_x, v_y = positions[v]\n",
        "            \n",
        "            mid_x = (u_x + v_x) / 2\n",
        "            mid_y = (u_y + v_y) / 2\n",
        "            \n",
        "            edge_hash = hash((u, v)) % 10\n",
        "            offset_x = 0.1 * (edge_hash % 3 - 1)\n",
        "            offset_y = 0.1 * ((edge_hash // 3) % 3 - 1)\n",
        "            \n",
        "            edge_labels_pos[(u, v)] = (mid_x + offset_x, mid_y + offset_y)\n",
        "        \n",
        "        for (u, v), label in edge_weights.items():\n",
        "            x, y = edge_labels_pos[(u, v)]\n",
        "            plt.text(x, y, label, fontsize=8, ha='center', va='center',\n",
        "                    bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
        "        \n",
        "        for layer_idx in range(len(self.layers)):\n",
        "            layer_name = \"Input Layer\" if layer_idx == 0 else \"Output Layer\" if layer_idx == len(self.layers) - 1 else f\"Hidden Layer {layer_idx}\"\n",
        "            plt.text(layer_idx * layer_spacing, 0.5, layer_name, fontsize=12, ha='center', bbox=dict(facecolor='white', alpha=0.5))\n",
        "            plt.text(layer_idx * layer_spacing, 0.25, f\"Layer {layer_idx}\", fontsize=10, ha='center', bbox=dict(facecolor='white', alpha=0.5))\n",
        "        \n",
        "        print(\"STRUKTUR JARINGAN\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_weight_distribution(self, layers_to_plot, plot_type='histogram'):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        if plot_type == 'histogram':\n",
        "            for layer_idx in layers_to_plot:\n",
        "                plt.hist(self.weights[layer_idx].flatten(), bins=20, alpha=0.5, label=f'Layer {layer_idx}')\n",
        "                \n",
        "        elif plot_type == 'line':\n",
        "            for layer_idx in layers_to_plot:\n",
        "                weights_flat = self.weights[layer_idx].flatten()\n",
        "                \n",
        "                density = stats.gaussian_kde(weights_flat)\n",
        "                \n",
        "                x_min, x_max = min(weights_flat), max(weights_flat)\n",
        "                x = np.linspace(x_min, x_max, 200)\n",
        "\n",
        "                plt.plot(x, density(x), label=f'Layer {layer_idx}')\n",
        "\n",
        "                plt.plot(weights_flat, np.zeros_like(weights_flat), '|', \n",
        "                        color=plt.gca().lines[-1].get_color(), alpha=0.3, markersize=5)\n",
        "        \n",
        "        plt.title(\"Weight Distribution\")\n",
        "        plt.xlabel(\"Weight Value\")\n",
        "        plt.ylabel(\"Density\" if plot_type == 'line' else \"Frequency\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_gradient_distribution(self, layers_to_plot, plot_type='histogram'):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        if plot_type == 'histogram':\n",
        "            for layer_idx in layers_to_plot:\n",
        "                plt.hist(self.gradients[layer_idx].flatten(), bins=20, alpha=0.5, label=f'Layer {layer_idx}')\n",
        "                \n",
        "        elif plot_type == 'line':\n",
        "            for layer_idx in layers_to_plot:\n",
        "                gradients_flat = self.gradients[layer_idx].flatten()\n",
        "\n",
        "                density = stats.gaussian_kde(gradients_flat)\n",
        "\n",
        "                x_min, x_max = min(gradients_flat), max(gradients_flat)\n",
        "                x = np.linspace(x_min, x_max, 200)\n",
        "\n",
        "                plt.plot(x, density(x), label=f'Layer {layer_idx}')\n",
        "\n",
        "                plt.plot(gradients_flat, np.zeros_like(gradients_flat), '|', \n",
        "                            color=plt.gca().lines[-1].get_color(), alpha=0.3, markersize=5)\n",
        "        \n",
        "        plt.title(\"Gradient Distribution\")\n",
        "        plt.xlabel(\"Gradient Value\")\n",
        "        plt.ylabel(\"Density\" if plot_type == 'line' else \"Frequency\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "layers = [3, 4, 2]\n",
        "\n",
        "custom_weights = [\n",
        "    np.random.normal(0, 0.5, (3, 4)),\n",
        "    np.random.normal(0, 0.5, (4, 2))\n",
        "]\n",
        "\n",
        "custom_gradients = [\n",
        "    np.random.normal(0, 0.1, (3, 4)),\n",
        "    np.random.normal(0, 0.1, (4, 2))\n",
        "]\n",
        "\n",
        "custom_biases = [\n",
        "    np.random.normal(0, 0.3),  # Biases for the first hidden layer\n",
        "    np.random.normal(0, 0.3)   # Biases for the output layer\n",
        "]\n",
        "\n",
        "visualizer = NeuralNetworkVisualizer(layers, \n",
        "                                   weights=custom_weights,\n",
        "                                   gradients=custom_gradients)\n",
        "\n",
        "visualizer.plot_network()\n",
        "\n",
        "visualizer.plot_weight_distribution([0, 1], plot_type='line')\n",
        "\n",
        "visualizer.plot_gradient_distribution([0, 1], plot_type='line')\n",
        "\n",
        "visualizer.plot_weight_distribution([0, 1], plot_type='histogram')\n",
        "\n",
        "visualizer.plot_weight_distribution([0])  # Plot only first layer weights\n",
        "visualizer.plot_gradient_distribution([1])  # Plot only output layer gradients"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
