{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR1JW69eLfG_"
      },
      "source": [
        "# IF3270 Pembelajaran Mesin Feedforward Neural Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucbaI5rBLtjJ"
      },
      "source": [
        "Group Number: 18\n",
        "\n",
        "Group Members:\n",
        "- Dhafin Fawwaz Ikramullah (13522084)\n",
        "- Raden Rafly Hanggaraksa B (13522014)\n",
        "- Saad Abdul Hakim (13522092)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzsfETHLfHA"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jZJU5W_4LfHB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.datasets import fetch_openml\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import check_random_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKbjLIdYLfHC"
      },
      "source": [
        "## Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IWFJ-gdLfHD"
      },
      "outputs": [],
      "source": [
        "# Example of reading a csv file from a gdrive link\n",
        "\n",
        "# Take the file id from the gdrive file url\n",
        "# https://drive.google.com/file/d/1ZUtiaty9RPXhpz5F2Sy3dFPHF4YIt5iU/view?usp=sharing => The file id is 1ZUtiaty9RPXhpz5F2Sy3dFPHF4YIt5iU\n",
        "# and then put it in this format:\n",
        "# https://drive.google.com/uc?id={file_id}\n",
        "# Don't forget to change the access to public\n",
        "\n",
        "# df = pd.read_csv('https://drive.google.com/uc?id=1ZUtiaty9RPXhpz5F2Sy3dFPHF4YIt5iU')\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sparsity with L1 penalty: 76.96%\n",
            "Test score with L1 penalty: 0.8370\n",
            "Example run in 12.363 s\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Authors: The scikit-learn developers\n",
        "# SPDX-License-Identifier: BSD-3-Clause\n",
        "\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "# Turn down for faster convergence\n",
        "t0 = time.time()\n",
        "train_samples = 5000\n",
        "\n",
        "# Load data from https://www.openml.org/d/554\n",
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
        "\n",
        "pd.DataFrame(X).to_csv('dataset/X.csv', index=False)\n",
        "pd.DataFrame(y).to_csv('dataset/y.csv', index=False)\n",
        "\n",
        "random_state = check_random_state(0)\n",
        "permutation = random_state.permutation(X.shape[0])\n",
        "X = X[permutation]\n",
        "y = y[permutation]\n",
        "X = X.reshape((X.shape[0], -1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, train_size=train_samples, test_size=10000\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Turn up tolerance for faster convergence\n",
        "clf = LogisticRegression(C=50.0 / train_samples, penalty=\"l1\", solver=\"saga\", tol=0.1)\n",
        "clf.fit(X_train, y_train)\n",
        "sparsity = np.mean(clf.coef_ == 0) * 100\n",
        "score = clf.score(X_test, y_test)\n",
        "# print('Best C % .4f' % clf.C_)\n",
        "print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
        "print(\"Test score with L1 penalty: %.4f\" % score)\n",
        "\n",
        "coef = clf.coef_.copy()\n",
        "plt.figure(figsize=(10, 5))\n",
        "scale = np.abs(coef).max()\n",
        "for k in range(10):\n",
        "    l1_plot = plt.subplot(2, 5, k + 1)\n",
        "    l1_plot.imshow(\n",
        "        coef[k].reshape(28, 28),\n",
        "        interpolation=\"nearest\",\n",
        "        cmap=plt.cm.RdBu,\n",
        "        vmin=-scale,\n",
        "        vmax=scale,\n",
        "    )\n",
        "    l1_plot.set_xticks(())\n",
        "    l1_plot.set_yticks(())\n",
        "    l1_plot.set_xlabel(\"Class %i\" % k)\n",
        "plt.suptitle(\"Classification vector for...\")\n",
        "\n",
        "run_time = time.time() - t0\n",
        "print(\"Example run in %.3f s\" % run_time)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_csv = pd.read_csv(\"dataset/X.csv\")\n",
        "y_csv = pd.read_csv(\"dataset/y.csv\")\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     X, y, train_size=train_samples, test_size=10000\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib.axes._axes import Axes\n",
        "\n",
        "def visualize(X, y, row_count, col_count, offset = 0):\n",
        "    # scale = np.abs(X).max()\n",
        "    scale = 255 # in case we only pick some data and none of them reach the max value (255). \n",
        "    fig, axes = plt.subplots(row_count, col_count, figsize=(10, 10))\n",
        "    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
        "\n",
        "    for i in range(row_count * col_count):\n",
        "        ax: Axes = axes[i // col_count, i % col_count]\n",
        "        \n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_xlabel(str(i+offset)+\": \"+str(int(y[i+offset])))\n",
        "        \n",
        "        ax.imshow(\n",
        "            X[i+offset].reshape(28, 28),\n",
        "            interpolation=\"nearest\",\n",
        "            cmap=plt.cm.RdBu,\n",
        "            vmin=-scale,\n",
        "            vmax=scale,\n",
        "        )\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(70000, 784)\n",
            "(70000,)\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'visualize' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(X_data.shape)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(y_data.shape)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mvisualize\u001b[49m(X_data, y_data, row_count, col_count, \u001b[32m0\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'visualize' is not defined"
          ]
        }
      ],
      "source": [
        "row_count = 10\n",
        "col_count = 10\n",
        "\n",
        "X_data = X_csv.to_numpy()\n",
        "y_data_temp = y_csv.to_numpy()\n",
        "y_data = np.zeros(len(y_data_temp))\n",
        "for k in range(len(y_data_temp)):\n",
        "    y_data[k] = y_data_temp[k][0]\n",
        "\n",
        "print(X_data.shape)\n",
        "print(y_data.shape)\n",
        "\n",
        "visualize(X_data, y_data, row_count, col_count, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## With Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "# X, y = make_classification(n_samples=100, random_state=1)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\n",
        "clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
        "clf.predict_proba(X_test[:1])\n",
        "print(clf.score(X_test, y_test))\n",
        "# X_MLP = X_test[0:5]\n",
        "# y_MLP = clf.predict(X_MLP)\n",
        "# visualize(X_MLP, y_MLP, 1, 3)\n",
        "# print(X_MLP)\n",
        "# print(y_MLP)\n",
        "\n",
        "\n",
        "y_MLP = clf.predict(X_data)\n",
        "print(X_data.shape)\n",
        "print(y_MLP.shape)\n",
        "visualize(X_data, y_MLP, 10, 10, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "differences = 0\n",
        "for k in range(len(y_data)):\n",
        "    if int(y_data[k]) != int(y_MLP[k]):\n",
        "        differences += 1\n",
        "print(differences)\n",
        "\n",
        "print(str((1-differences/len(y_data)) * 100) + \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Literal, Callable, Union\n",
        "import numpy as np\n",
        "from numpy.typing import NDArray, ArrayLike\n",
        "\n",
        "\n",
        "class FFNNClassifier:\n",
        "    def __init__(self, \n",
        "            hidden_layer_sizes: NDArray,\n",
        "            activation_func: Literal['linear', 'relu', 'sigmoid', 'tanh', 'softmax'], \n",
        "            learning_rate: float,\n",
        "            verbose: int, # 0: no print, 1: print epoch progress\n",
        "            max_epoch: int,\n",
        "            batch_size: int,\n",
        "            loss_func: Literal['mean_squared_error', 'binary_cross_entropy', 'categorical_cross_entropy']\n",
        "        ):\n",
        "        self.hidden_layer_sizes = hidden_layer_sizes\n",
        "        self.X: NDArray = []\n",
        "        self.y: list[ArrayLike] = []\n",
        "        self.weights_history: list[NDArray] = [] # array of weight matrix. index is current epoch\n",
        "        self.biases_history: list[ArrayLike] = [] # array of bias list. index is current epoch\n",
        "        self.weight_gradients_history: list[NDArray] = [] # array of weight gradients. index is current epoch\n",
        "\n",
        "        self.activation_func = activation_func\n",
        "        self.learning_rate = learning_rate\n",
        "        self.verbose = verbose\n",
        "        self.epoch_amount = max_epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.loss_func = loss_func\n",
        "\n",
        "    # return [ matrix, matrix, matrix ... ] where matrix is the weight adjacency matrix for each layer. length should be number of layers - 1 because its like the edges/connection between the nodes\n",
        "    def _generate_initial_weights(self):\n",
        "        \n",
        "        # delete this\n",
        "        weights, nodes, nodes_active, biases = self._generate_new_empty_layers()\n",
        "        return weights\n",
        "        # return [np.array([[0.15, 0.25], [0.2, 0.3]]), np.array([[0.4, 0.5], [0.45, 0.55]])]\n",
        "    \n",
        "    # return [ float, float, float ... ] where float is the bias for each layer. length should be number of layers - 1 because input layer does not have bias\n",
        "    def _generate_initial_biases(self):\n",
        "        \n",
        "        # delete this\n",
        "        weights, nodes, nodes_active, biases = self._generate_new_empty_layers()\n",
        "        return biases\n",
        "        # return [np.array([0.35, 0.35]), np.array([0.6, 0.6])]\n",
        "        \n",
        "\n",
        "# region functions\n",
        "    def _activation_function(self, x: Union[float, NDArray], func: str):\n",
        "        if func == 'linear': return x\n",
        "        elif func == 'relu': return np.maximum(0, x)\n",
        "        elif func == 'sigmoid': return 1.0/(1.0 + np.exp(-x))\n",
        "        elif func == 'tanh': return np.tanh(x)\n",
        "        elif func == 'softmax':\n",
        "            exp_x = np.exp(x - np.max(x, axis=1, keepdims=True)) \n",
        "            return exp_x / np.sum(exp_x, axis=1, keepdims=True) # keepdims=True will keep the dimension of the original array\n",
        "        raise \"Activation function not supported!\"\n",
        "    \n",
        "    def _activation_derived_function(self, x: Union[float, NDArray], func: str):\n",
        "        if func == 'linear': return np.ones_like(x)\n",
        "        elif func == 'relu': return np.where(x > 0, 1, 0)\n",
        "        elif func == 'sigmoid':\n",
        "            sig = self._activation_function(x, 'sigmoid')\n",
        "            return sig * (1 - sig)\n",
        "        elif func == 'tanh': \n",
        "            p = 2.0/(np.exp(x) - np.exp(-x)) # should be the same as 1 - np.tanh(x) ** 2. will check later\n",
        "            return p*p\n",
        "        elif func == 'softmax': # TODO: Check if this implementation is correct\n",
        "            delta = np.zeros_like(x) \n",
        "            for i in range(x.shape[0]): delta[i, i] = 1\n",
        "            softmax = self._activation_function(x, 'softmax')\n",
        "            return softmax*(delta-softmax) \n",
        "            # s = self._activation_function(x, 'softmax')\n",
        "            # return s * (1 - s)\n",
        "        raise \"Activation function not supported!\"\n",
        "    \n",
        "\n",
        "    def _loss_function(self, y_act, y_pred, number_of_classes, func: str):\n",
        "        if func == 'mean_squared_error': return FFNNClassifier.mean_squared_error(y_act, y_pred)\n",
        "        elif func == 'binary_cross_entropy': return FFNNClassifier.binary_cross_entropy(y_act, y_pred)\n",
        "        elif func == 'categorical_cross_entropy': return FFNNClassifier.categorical_cross_entropy(y_act, y_pred, number_of_classes)\n",
        "    def mean_squared_error(y_act, y_pred):\n",
        "        return 2/len(y_act[0]) * (y_pred - y_act)\n",
        "    def binary_cross_entropy(y_act, y_pred, n):\n",
        "        return (y_pred - y_act)/(y_pred*(1 - y_pred))\n",
        "    def categorical_cross_entropy(y_act, y_pred, c):\n",
        "        return y_pred - y_act\n",
        "\n",
        "# endregion functions\n",
        "\n",
        "\n",
        "# region getters setters\n",
        "    \n",
        "    # Can only be called after setting X and y\n",
        "    def _get_hidden_layer_sizes(self) -> np.typing.NDArray:\n",
        "        \n",
        "        len_features = len(self.X[0])\n",
        "        len_classes = self._get_number_of_classes()\n",
        "        layer_sizes = np.zeros(len(self.hidden_layer_sizes)+2, dtype=int)\n",
        "        layer_sizes[0] = len_features\n",
        "        for i in range(1, len(self.hidden_layer_sizes)+1):\n",
        "            layer_sizes[i] = self.hidden_layer_sizes[i-1]\n",
        "        layer_sizes[len(layer_sizes)-1] = len_classes\n",
        "        \n",
        "        return layer_sizes\n",
        "    \n",
        "    # Can only be called after setting X and y\n",
        "    def _generate_new_empty_layers(self):\n",
        "        \n",
        "        layer_sizes = self._get_hidden_layer_sizes()\n",
        "        network_depth = len(layer_sizes)\n",
        "        weights = []\n",
        "        biases = []\n",
        "        nodes = []\n",
        "        nodes_active = []\n",
        "        for i in range(network_depth-1):\n",
        "            weights.append(np.zeros((layer_sizes[i], layer_sizes[i+1])))\n",
        "            biases.append(np.zeros(layer_sizes[i+1]))\n",
        "        for i in range(network_depth):\n",
        "            nodes.append(np.zeros(layer_sizes[i]))\n",
        "            nodes_active.append(np.zeros(layer_sizes[i]))\n",
        "        \n",
        "        return weights, nodes, nodes_active, biases\n",
        "\n",
        "\n",
        "    # Can only be called after setting X and y\n",
        "    def _get_number_of_classes(self):\n",
        "        # return len(np.unique(self.y)) \n",
        "        return 10 # hardcoded because it messes up things when the possible value is not much\n",
        "    \n",
        "    def copy_list_as_zeros(self, list: list[NDArray]):\n",
        "        return [np.zeros_like(w) for w in list]\n",
        "    \n",
        "# endregion getters setters\n",
        "\n",
        "    \n",
        "\n",
        "    def fit(self, X: NDArray, y: NDArray):\n",
        "        if type(y) == list and type(y[0]) != list:\n",
        "            y = np.array([[i] for i in y])\n",
        "        if type(y) == list and type(y[0]) == list:\n",
        "            y = np.array(y)\n",
        "\n",
        "        if len(X) != len(y):\n",
        "            raise Exception(\"length of X and y is not the same\")\n",
        "        if len(X) == 0:\n",
        "            raise Exception(\"len(self.X) == 0\")\n",
        "        if len(X[0]) == 0:\n",
        "            raise Exception(\"len(self.X[0]) == 0\")\n",
        "        if len(y) == 0:\n",
        "            raise Exception(\"len(self.y) == 0\")\n",
        "        \n",
        "        # clean up in case this function is called multiple times\n",
        "        self.X: NDArray = []\n",
        "        self.y: ArrayLike = []\n",
        "        self.weights_history: list[NDArray] = []\n",
        "        self.biases_history: list[ArrayLike] = []\n",
        "        self.weight_gradients_history: list[NDArray] = []\n",
        "\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        initial_weight = self._generate_initial_weights()\n",
        "        initial_bias = self._generate_initial_biases()\n",
        "        self.weights_history = initial_weight\n",
        "        self.biases_history = initial_bias\n",
        "        initial_gradients = [np.zeros_like(w) for w in initial_weight]\n",
        "        self.weight_gradients_history.append(initial_gradients)\n",
        "\n",
        "        layer_sizes = self._get_hidden_layer_sizes()\n",
        "        network_depth = len(layer_sizes)\n",
        "        number_of_classes = self._get_number_of_classes()\n",
        "\n",
        "        for epoch in range(self.epoch_amount):\n",
        "            # for current_dataset_idx in range(len(self.X)):\n",
        "            current_dataset_idx = 0\n",
        "            while current_dataset_idx < len(self.X):\n",
        "                weights, nodes, nodes_active, biases = self._generate_new_empty_layers() # will be filled with weights based on the previous epoch. Will be appended to the history after the end of the epoch\n",
        "\n",
        "                # Feed Forward\n",
        "                until_idx = min(current_dataset_idx+self.batch_size, len(self.X))\n",
        "                nodes[0] = self.X[current_dataset_idx:until_idx]\n",
        "                nodes_active[0] = self.X[current_dataset_idx:until_idx] # not passed to activation function for the first layer\n",
        "\n",
        "                for k in range(1, network_depth):\n",
        "                    w_k = self.weights_history[k-1]\n",
        "                    b_k = self.biases_history[k-1]\n",
        "                    h_k_min_1 = nodes_active[k-1]\n",
        "\n",
        "                    a_k = b_k + np.dot(h_k_min_1, w_k) # numpy will automatically broadcast b_k (row will be copied to match the result from dot) so that this is addable\n",
        "\n",
        "                    nodes[k] = a_k\n",
        "                    nodes_active[k] = self._activation_function(a_k, self.activation_func)\n",
        "                \n",
        "                # print([p.shape for p in nodes_active])\n",
        "                loss_grad = self._loss_function(\n",
        "                    y_act=self.y[current_dataset_idx:until_idx], \n",
        "                    y_pred=nodes_active[network_depth-1], \n",
        "                    number_of_classes=number_of_classes, \n",
        "                    func=self.loss_func\n",
        "                )\n",
        "                # print(\"self.y[current_dataset_idx:until_idx]: \", self.y[current_dataset_idx:until_idx])\n",
        "                # print(\"nodes_active[network_depth-1]: \", nodes_active[network_depth-1])\n",
        "                # print(\"loss_grad: \", loss_grad)\n",
        "\n",
        "\n",
        "                # Backward Propagation\n",
        "                weight_gradiens = [0 for i in range(len(self.weights_history))] # 0 will be replaced with numpy.array\n",
        "                bias_gradiens = [0 for i in range(len(self.biases_history))] # 0 will be replaced with numpy.array\n",
        "                \n",
        "                delta = loss_grad * self._activation_derived_function(nodes[-1], self.activation_func)\n",
        "\n",
        "                weight_gradiens[network_depth-2] = np.dot(nodes_active[-2].T, -delta)\n",
        "                bias_gradiens[network_depth-2] = -delta\n",
        "                \n",
        "                \n",
        "                for k in range(network_depth-2, 0, -1): # from the last hidden layer (not including the output layer)\n",
        "                    w = self.weights_history[k]\n",
        "\n",
        "                    delta = np.dot(delta, w.T) * self._activation_derived_function(nodes[k], self.activation_func)\n",
        "\n",
        "                    weight_gradiens[k-1] = np.dot(nodes_active[k-1].T, -delta)\n",
        "                    bias_gradiens[k-1] = -delta\n",
        "                \n",
        "                self.weight_gradients_history.append(weight_gradiens)\n",
        "                \n",
        "                # Update\n",
        "                for k in range(network_depth-1):\n",
        "                    w_k = self.weights_history[k]\n",
        "                    b_k = self.biases_history[k]\n",
        "\n",
        "                    weights[k] = w_k + self.learning_rate * weight_gradiens[k]\n",
        "\n",
        "                    biases[k] = b_k + self.learning_rate * bias_gradiens[k]\n",
        "                self.weights_history = weights\n",
        "                self.biases_history = biases\n",
        "\n",
        "            \n",
        "                current_dataset_idx += self.batch_size\n",
        "            #### while loop ends here ##############################################\n",
        "\n",
        "\n",
        "            if self.verbose == 1:\n",
        "                print(f\"Epoch {epoch+1}/{self.epoch_amount} done\")\n",
        "            elif self.verbose == 2:\n",
        "                print(f\"========================================\")\n",
        "                print(f\"Epoch {epoch+1}/{self.epoch_amount} done\")\n",
        "                print(f\"weights: {self.weights_history}\")\n",
        "                print(f\"biases: {self.biases_history}\")\n",
        "\n",
        "\n",
        "    def predict(self, X_test: NDArray):\n",
        "        prediction = [-1 for i in range(len(X_test))]\n",
        "        current_idx = 0\n",
        "        while current_idx < len(X_test):\n",
        "            weights, nodes, nodes_active, biases = self._generate_new_empty_layers()\n",
        "            until_idx = min(current_idx+self.batch_size, len(X_test))\n",
        "            nodes[0] = X_test[current_idx:until_idx]\n",
        "            nodes_active[0] = X_test[current_idx:until_idx]\n",
        "\n",
        "            for k in range(1, len(self.weights_history)+1):\n",
        "                w_k = self.weights_history[k-1]\n",
        "                b_k = self.biases_history[k-1]\n",
        "                h_k_min_1 = nodes_active[k-1]\n",
        "\n",
        "                a_k = b_k + np.dot(h_k_min_1, w_k)\n",
        "\n",
        "                nodes[k] = a_k\n",
        "                nodes_active[k] = self._activation_function(a_k, self.activation_func)\n",
        "            predicted_class = [int(np.argmax(nodes_active[-1][i])) for i in range(len(nodes_active[-1]))] # idx with highest value. idx is also the class\n",
        "            prediction[current_idx:until_idx] = predicted_class\n",
        "            current_idx += self.batch_size\n",
        "            \n",
        "        return prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'FFNNClassifier' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Case edunex\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ffnn = \u001b[43mFFNNClassifier\u001b[49m(\n\u001b[32m      4\u001b[39m     hidden_layer_sizes=[\u001b[32m3\u001b[39m],\n\u001b[32m      5\u001b[39m     activation_func=\u001b[33m\"\u001b[39m\u001b[33msigmoid\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     learning_rate=\u001b[32m0.5\u001b[39m,\n\u001b[32m      7\u001b[39m     verbose=\u001b[32m1\u001b[39m,\n\u001b[32m      8\u001b[39m     max_epoch=\u001b[32m1\u001b[39m,\n\u001b[32m      9\u001b[39m     batch_size=\u001b[32m1\u001b[39m,\n\u001b[32m     10\u001b[39m     loss_func=\u001b[33m\"\u001b[39m\u001b[33mmean_squared_error\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_initial_weights\u001b[39m():\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [np.array([[\u001b[32m0.15\u001b[39m, \u001b[32m0.25\u001b[39m], [\u001b[32m0.2\u001b[39m, \u001b[32m0.3\u001b[39m]]), np.array([[\u001b[32m0.4\u001b[39m, \u001b[32m0.5\u001b[39m], [\u001b[32m0.45\u001b[39m, \u001b[32m0.55\u001b[39m]])]\n",
            "\u001b[31mNameError\u001b[39m: name 'FFNNClassifier' is not defined"
          ]
        }
      ],
      "source": [
        "# Case edunex\n",
        "\n",
        "ffnn = FFNNClassifier(\n",
        "    hidden_layer_sizes=[3],\n",
        "    activation_func=\"sigmoid\",\n",
        "    learning_rate=0.5,\n",
        "    verbose=1,\n",
        "    max_epoch=1,\n",
        "    batch_size=1,\n",
        "    loss_func=\"mean_squared_error\"\n",
        ")\n",
        "def _generate_initial_weights():\n",
        "    return [np.array([[0.15, 0.25], [0.2, 0.3]]), np.array([[0.4, 0.5], [0.45, 0.55]])]\n",
        "def _generate_initial_biases():\n",
        "    return [np.array([0.35, 0.35]), np.array([0.6, 0.6])]\n",
        "ffnn._generate_initial_weights = _generate_initial_weights\n",
        "ffnn._generate_initial_biases = _generate_initial_biases\n",
        "\n",
        "X_temp = np.array([[0.05, 0.1]])\n",
        "y_temp = np.array([[0.01, 0.99]])\n",
        "ffnn.fit(X_temp, y_temp)\n",
        "# prediction = ffnn.predict(X_temp)\n",
        "# print(prediction)\n",
        "\n",
        "print(\"Final Weights:\", ffnn.weights_history)\n",
        "print(\"Final Biases:\", ffnn.biases_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 done\n",
            "Epoch 2/5 done\n",
            "Epoch 3/5 done\n",
            "Epoch 4/5 done\n",
            "Epoch 5/5 done\n",
            "Prediction: [1, 1, 1, 1]\n",
            "Final Weights: [array([[4.34367662e-09, 4.34367662e-09, 4.34367662e-09],\n",
            "       [8.68735324e-09, 8.68735324e-09, 8.68735324e-09],\n",
            "       [1.73747065e-08, 1.73747065e-08, 1.73747065e-08],\n",
            "       [2.17183831e-08, 2.17183831e-08, 2.17183831e-08]]), array([[0.00045716, 0.00045716],\n",
            "       [0.00045716, 0.00045716],\n",
            "       [0.00045716, 0.00045716]]), array([[0.02875542, 0.02875542, 0.02875542, 0.02875542, 0.02875542],\n",
            "       [0.02875542, 0.02875542, 0.02875542, 0.02875542, 0.02875542]]), array([[-0.36255151,  0.36255151],\n",
            "       [-0.36255151,  0.36255151],\n",
            "       [-0.36255151,  0.36255151],\n",
            "       [-0.36255151,  0.36255151],\n",
            "       [-0.36255151,  0.36255151]])]\n",
            "Final Biases: [array([[4.34367662e-08, 4.34367662e-08, 4.34367662e-08],\n",
            "       [4.34367662e-08, 4.34367662e-08, 4.34367662e-08]]), array([[0.00045716, 0.00045716],\n",
            "       [0.00045716, 0.00045716]]), array([[0.0287512, 0.0287512, 0.0287512, 0.0287512, 0.0287512],\n",
            "       [0.0287512, 0.0287512, 0.0287512, 0.0287512, 0.0287512]]), array([[-0.35988306,  0.35988306],\n",
            "       [-0.35988306,  0.35988306]])]\n"
          ]
        }
      ],
      "source": [
        "# Case random\n",
        "\n",
        "ffnn = FFNNClassifier(\n",
        "    hidden_layer_sizes=[3,2,5],\n",
        "    activation_func=\"sigmoid\",\n",
        "    learning_rate=0.5,\n",
        "    verbose=1,\n",
        "    max_epoch=5,\n",
        "    batch_size=2,\n",
        "    loss_func=\"mean_squared_error\"\n",
        ")\n",
        "\n",
        "X_temp = np.array([[0.05, 0.1, 0.2, 0.25], [0.05, 0.1, 0.2, 0.25], [0.05, 0.1, 0.2, 0.25], [0.05, 0.1, 0.2, 0.25]])\n",
        "y_temp = np.array([[0.01, 0.99], [0.01, 0.99], [0.01, 0.99], [0.01, 0.99]])\n",
        "ffnn.fit(X_temp, y_temp)\n",
        "prediction = ffnn.predict(X_temp)\n",
        "print(\"Prediction:\", prediction)\n",
        "print(\"Final Weights:\", ffnn.weights_history)\n",
        "print(\"Final Biases:\", ffnn.biases_history[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15 done\n",
            "Epoch 2/15 done\n",
            "Epoch 3/15 done\n",
            "Epoch 4/15 done\n",
            "Epoch 5/15 done\n",
            "Epoch 6/15 done\n",
            "Epoch 7/15 done\n",
            "Epoch 8/15 done\n",
            "Epoch 9/15 done\n",
            "Epoch 10/15 done\n",
            "Epoch 11/15 done\n",
            "Epoch 12/15 done\n",
            "Epoch 13/15 done\n",
            "Epoch 14/15 done\n",
            "Epoch 15/15 done\n",
            "Prediction: [3, 7, 1, 0, 7, 2, 0, 2, 0, 0, 1, 3, 3, 1, 2, 0, 7, 6, 7, 0, 3, 3, 2, 0, 2, 1, 7, 2, 3, 0, 1, 1, 0, 3, 3, 0, 3, 0, 7, 2, 6, 3, 2, 2, 0, 1, 0, 7, 7, 7, 3, 7, 1, 0, 7, 2, 0, 2, 0, 0, 1, 3, 3, 1, 2, 0, 7, 6, 7, 0, 3, 3, 2, 0, 2, 1, 7, 2, 3, 0, 1, 1, 0, 3, 3, 0, 3, 0, 7, 2, 6, 3, 2, 2, 0, 1, 0, 7, 7, 7]\n",
            "Expected: [5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5, 9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0, 4, 5, 6, 1, 0, 0, 1, 7, 1, 6, 3, 0, 2, 1, 1, 7, 9, 0, 2, 6, 7, 8, 3, 9, 0, 4, 6, 7, 4, 6, 8, 0, 7, 8, 3, 1]\n",
            "Accuracy: 9.0 %\n"
          ]
        }
      ],
      "source": [
        "# Case random\n",
        "def one_hot_encode(y):\n",
        "    num_of_classes = 10\n",
        "    arr = []\n",
        "    for i in range(len(y)):\n",
        "        arr.append([0 for j in range(num_of_classes)]) # hardcoded 10 for the number of classes\n",
        "        arr[i][y[i]] = 1\n",
        "    return arr\n",
        "\n",
        "\n",
        "ffnn = FFNNClassifier(\n",
        "    hidden_layer_sizes=[256, 128, 64],\n",
        "    activation_func=\"sigmoid\",\n",
        "    learning_rate=0.05,\n",
        "    verbose=1,\n",
        "    max_epoch=15,\n",
        "    batch_size=50,\n",
        "    loss_func=\"mean_squared_error\"\n",
        ")\n",
        "\n",
        "train_until_idx = 10000\n",
        "test_until_idx = 100\n",
        "x_sliced = X_data[0:train_until_idx]\n",
        "y_sliced = y_data[0:train_until_idx]\n",
        "y_sliced = [int(i) for i in y_sliced]\n",
        "y_one_hot = one_hot_encode([int(i) for i in y_sliced])\n",
        "\n",
        "# print(x_sliced)\n",
        "# print(y_sliced)\n",
        "\n",
        "ffnn.fit(x_sliced, y_one_hot)\n",
        "prediction = ffnn.predict(X_data[0:test_until_idx])\n",
        "print(\"Prediction:\", prediction)\n",
        "print(\"Expected:\", y_sliced[0:test_until_idx])\n",
        "# print(\"Final Weights:\", ffnn.weights_history)\n",
        "# print(\"Final Biases:\", ffnn.biases_history[-1])\n",
        "\n",
        "def get_same_count(y1, y2):\n",
        "    count = 0\n",
        "    for i in range(len(y1)):\n",
        "        if y1[i] == y2[i]:\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "print(\"Accuracy:\", get_same_count(prediction, y_sliced)/len(prediction) * 100, \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
            " ...\n",
            " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 ... 0.5 0.5 0.5]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "arrtest = np.array([[0.5 for j in range(512)] for i in range(784)])\n",
        "print(arrtest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      3\u001b[39m ffnn = MLPClassifier(\n\u001b[32m      4\u001b[39m     hidden_layer_sizes=(\u001b[32m3\u001b[39m),\n\u001b[32m      5\u001b[39m     activation=\u001b[33m\"\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# loss_func=\"mean_squared_error\"\u001b[39;00m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     13\u001b[39m y_target = [\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m3\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m ffnn.fit(\u001b[43mX_train\u001b[49m[\u001b[32m0\u001b[39m:\u001b[32m3\u001b[39m], y_train[\u001b[32m0\u001b[39m:\u001b[32m3\u001b[39m])\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# prediction = ffnn.predict(X_temp)\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "ffnn = MLPClassifier(\n",
        "    hidden_layer_sizes=(3),\n",
        "    activation=\"relu\",\n",
        "    learning_rate=\"constant\",\n",
        "    learning_rate_init=0.5,\n",
        "    verbose=0,\n",
        "    max_iter=500,\n",
        "    batch_size=2,\n",
        "    # loss_func=\"mean_squared_error\"\n",
        ")\n",
        "y_target = [0,1,2,3]\n",
        "print(y_target)\n",
        "ffnn.fit(X_temp, y_target)\n",
        "# prediction = ffnn.predict(X_temp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from scipy import stats\n",
        "\n",
        "class NeuralNetworkVisualizer:\n",
        "    def __init__(self, layers, weights=None, gradients=None, biases=None):\n",
        "        self.layers = layers\n",
        "\n",
        "        if weights is not None:\n",
        "            self.weights = [w[0] if isinstance(w, list) and isinstance(w[0], np.ndarray) else w for w in weights]\n",
        "        else:\n",
        "            self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers) - 1)]\n",
        "\n",
        "        if gradients is not None:\n",
        "            self.gradients = [g[0] if isinstance(g, list) and isinstance(g[0], np.ndarray) else g for g in gradients]\n",
        "        else:\n",
        "            self.gradients = [np.random.randn(*w.shape) * 0.1 for w in self.weights]\n",
        "            \n",
        "        if biases is not None:\n",
        "            self.biases = [b[0] if isinstance(b, list) and isinstance(b[0], np.ndarray) else b for b in biases]\n",
        "        else:\n",
        "            self.biases = [np.random.randn(layers[i+1]) for i in range(len(layers) - 1)]\n",
        "\n",
        "    def plot_network(self):\n",
        "        G = nx.DiGraph()\n",
        "        positions = {}\n",
        "        node_count = 0\n",
        "        layer_spacing = 3\n",
        "        node_spacing = 1\n",
        "        colors = []\n",
        "        node_labels = {}\n",
        "\n",
        "        for layer_idx, num_nodes in enumerate(self.layers):\n",
        "            layer_start_node = node_count\n",
        "            for node_idx in range(num_nodes):\n",
        "                node_id = node_count\n",
        "                positions[node_id] = (layer_idx * layer_spacing, -node_idx * node_spacing)\n",
        "                G.add_node(node_id)\n",
        "                node_labels[node_id] = f\"{layer_idx},{node_idx}\"\n",
        "                if layer_idx == 0:\n",
        "                    colors.append(\"red\")  # Input layer\n",
        "                elif layer_idx == len(self.layers) - 1:\n",
        "                    colors.append(\"green\")  # Output layer\n",
        "                else:\n",
        "                    colors.append(\"blue\")  # Hidden layer\n",
        "                node_count += 1\n",
        "\n",
        "        edge_weights = {}\n",
        "        node_count = 0\n",
        "        \n",
        "        for layer_idx in range(len(self.layers) - 1):\n",
        "            layer_size = self.layers[layer_idx]\n",
        "            next_layer_size = self.layers[layer_idx + 1]\n",
        "            next_layer_start = node_count + layer_size\n",
        "            \n",
        "            for src in range(node_count, node_count + layer_size):\n",
        "                for dst_idx, dst in enumerate(range(next_layer_start, next_layer_start + next_layer_size)):\n",
        "                    weight = self.weights[layer_idx][src - node_count, dst_idx]\n",
        "                    G.add_edge(src, dst, weight=weight)\n",
        "                    edge_weights[(src, dst)] = f'{weight:.2f}'\n",
        "            \n",
        "            node_count += layer_size\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        \n",
        "        nx.draw(G, pos=positions, with_labels=False, node_size=500, node_color=colors, edge_color=\"gray\")\n",
        "        \n",
        "        edge_labels_pos = {}\n",
        "        for (u, v), label in edge_weights.items():\n",
        "            u_x, u_y = positions[u]\n",
        "            v_x, v_y = positions[v]\n",
        "            \n",
        "            mid_x = (u_x + v_x) / 2\n",
        "            mid_y = (u_y + v_y) / 2\n",
        "            \n",
        "            edge_hash = hash((u, v)) % 10\n",
        "            offset_x = 0.1 * (edge_hash % 3 - 1)\n",
        "            offset_y = 0.1 * ((edge_hash // 3) % 3 - 1)\n",
        "            \n",
        "            edge_labels_pos[(u, v)] = (mid_x + offset_x, mid_y + offset_y)\n",
        "        \n",
        "        for (u, v), label in edge_weights.items():\n",
        "            x, y = edge_labels_pos[(u, v)]\n",
        "            plt.text(x, y, label, fontsize=8, ha='center', va='center',\n",
        "                    bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
        "        \n",
        "        for layer_idx in range(len(self.layers)):\n",
        "            layer_name = \"Input Layer\" if layer_idx == 0 else \"Output Layer\" if layer_idx == len(self.layers) - 1 else f\"Hidden Layer {layer_idx}\"\n",
        "            plt.text(layer_idx * layer_spacing, 0.5, layer_name, fontsize=12, ha='center', bbox=dict(facecolor='white', alpha=0.5))\n",
        "            plt.text(layer_idx * layer_spacing, 0.25, f\"Layer {layer_idx}\", fontsize=10, ha='center', bbox=dict(facecolor='white', alpha=0.5))\n",
        "        \n",
        "        print(\"STRUKTUR JARINGAN\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_weight_distribution(self, layers_to_plot, plot_type='histogram'):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        if plot_type == 'histogram':\n",
        "            for layer_idx in layers_to_plot:\n",
        "                plt.hist(self.weights[layer_idx].flatten(), bins=20, alpha=0.5, label=f'Layer {layer_idx}')\n",
        "                \n",
        "        elif plot_type == 'line':\n",
        "            for layer_idx in layers_to_plot:\n",
        "                weights_flat = self.weights[layer_idx].flatten()\n",
        "                \n",
        "                density = stats.gaussian_kde(weights_flat)\n",
        "                \n",
        "                x_min, x_max = min(weights_flat), max(weights_flat)\n",
        "                x = np.linspace(x_min, x_max, 200)\n",
        "\n",
        "                plt.plot(x, density(x), label=f'Layer {layer_idx}')\n",
        "\n",
        "                plt.plot(weights_flat, np.zeros_like(weights_flat), '|', \n",
        "                        color=plt.gca().lines[-1].get_color(), alpha=0.3, markersize=5)\n",
        "        \n",
        "        plt.title(\"Weight Distribution\")\n",
        "        plt.xlabel(\"Weight Value\")\n",
        "        plt.ylabel(\"Density\" if plot_type == 'line' else \"Frequency\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_gradient_distribution(self, layers_to_plot, plot_type='histogram'):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        if plot_type == 'histogram':\n",
        "            for layer_idx in layers_to_plot:\n",
        "                plt.hist(self.gradients[layer_idx].flatten(), bins=20, alpha=0.5, label=f'Layer {layer_idx}')\n",
        "                \n",
        "        elif plot_type == 'line':\n",
        "            for layer_idx in layers_to_plot:\n",
        "                gradients_flat = self.gradients[layer_idx].flatten()\n",
        "\n",
        "                density = stats.gaussian_kde(gradients_flat)\n",
        "\n",
        "                x_min, x_max = min(gradients_flat), max(gradients_flat)\n",
        "                x = np.linspace(x_min, x_max, 200)\n",
        "\n",
        "                plt.plot(x, density(x), label=f'Layer {layer_idx}')\n",
        "\n",
        "                plt.plot(gradients_flat, np.zeros_like(gradients_flat), '|', \n",
        "                            color=plt.gca().lines[-1].get_color(), alpha=0.3, markersize=5)\n",
        "        \n",
        "        plt.title(\"Gradient Distribution\")\n",
        "        plt.xlabel(\"Gradient Value\")\n",
        "        plt.ylabel(\"Density\" if plot_type == 'line' else \"Frequency\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "layers = [3, 4, 2]\n",
        "\n",
        "custom_weights = [\n",
        "    np.random.normal(0, 0.5, (3, 4)),\n",
        "    np.random.normal(0, 0.5, (4, 2))\n",
        "]\n",
        "\n",
        "custom_gradients = [\n",
        "    np.random.normal(0, 0.1, (3, 4)),\n",
        "    np.random.normal(0, 0.1, (4, 2))\n",
        "]\n",
        "\n",
        "custom_biases = [\n",
        "    np.random.normal(0, 0.3),  # Biases for the first hidden layer\n",
        "    np.random.normal(0, 0.3)   # Biases for the output layer\n",
        "]\n",
        "\n",
        "visualizer = NeuralNetworkVisualizer(layers, \n",
        "                                   weights=custom_weights,\n",
        "                                   gradients=custom_gradients)\n",
        "\n",
        "visualizer.plot_network()\n",
        "\n",
        "visualizer.plot_weight_distribution([0, 1], plot_type='line')\n",
        "\n",
        "visualizer.plot_gradient_distribution([0, 1], plot_type='line')\n",
        "\n",
        "visualizer.plot_weight_distribution([0, 1], plot_type='histogram')\n",
        "\n",
        "visualizer.plot_weight_distribution([0])  # Plot only first layer weights\n",
        "visualizer.plot_gradient_distribution([1])  # Plot only output layer gradients"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
