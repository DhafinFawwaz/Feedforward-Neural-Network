{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR1JW69eLfG_"
      },
      "source": [
        "# IF3270 Pembelajaran Mesin Feedforward Neural Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucbaI5rBLtjJ"
      },
      "source": [
        "Group Number: 18\n",
        "\n",
        "Group Members:\n",
        "- Dhafin Fawwaz Ikramullah (13522084)\n",
        "- Raden Rafly Hanggaraksa B (13522014)\n",
        "- Saad Abdul Hakim (13522092)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzsfETHLfHA"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZJU5W_4LfHB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.datasets import fetch_openml\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import check_random_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKbjLIdYLfHC"
      },
      "source": [
        "## Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IWFJ-gdLfHD"
      },
      "outputs": [],
      "source": [
        "# Example of reading a csv file from a gdrive link\n",
        "\n",
        "# Take the file id from the gdrive file url\n",
        "# https://drive.google.com/file/d/1ZUtiaty9RPXhpz5F2Sy3dFPHF4YIt5iU/view?usp=sharing => The file id is 1ZUtiaty9RPXhpz5F2Sy3dFPHF4YIt5iU\n",
        "# and then put it in this format:\n",
        "# https://drive.google.com/uc?id={file_id}\n",
        "# Don't forget to change the access to public\n",
        "\n",
        "# df = pd.read_csv('https://drive.google.com/uc?id=1ZUtiaty9RPXhpz5F2Sy3dFPHF4YIt5iU')\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authors: The scikit-learn developers\n",
        "# SPDX-License-Identifier: BSD-3-Clause\n",
        "\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "# Turn down for faster convergence\n",
        "t0 = time.time()\n",
        "train_samples = 5000\n",
        "\n",
        "# Load data from https://www.openml.org/d/554\n",
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
        "\n",
        "pd.DataFrame(X).to_csv('dataset/X.csv', index=False)\n",
        "pd.DataFrame(y).to_csv('dataset/y.csv', index=False)\n",
        "\n",
        "random_state = check_random_state(0)\n",
        "permutation = random_state.permutation(X.shape[0])\n",
        "X = X[permutation]\n",
        "y = y[permutation]\n",
        "X = X.reshape((X.shape[0], -1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, train_size=train_samples, test_size=10000\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Turn up tolerance for faster convergence\n",
        "clf = LogisticRegression(C=50.0 / train_samples, penalty=\"l1\", solver=\"saga\", tol=0.1)\n",
        "clf.fit(X_train, y_train)\n",
        "sparsity = np.mean(clf.coef_ == 0) * 100\n",
        "score = clf.score(X_test, y_test)\n",
        "# print('Best C % .4f' % clf.C_)\n",
        "print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
        "print(\"Test score with L1 penalty: %.4f\" % score)\n",
        "\n",
        "coef = clf.coef_.copy()\n",
        "plt.figure(figsize=(10, 5))\n",
        "scale = np.abs(coef).max()\n",
        "for k in range(10):\n",
        "    l1_plot = plt.subplot(2, 5, k + 1)\n",
        "    l1_plot.imshow(\n",
        "        coef[k].reshape(28, 28),\n",
        "        interpolation=\"nearest\",\n",
        "        cmap=plt.cm.RdBu,\n",
        "        vmin=-scale,\n",
        "        vmax=scale,\n",
        "    )\n",
        "    l1_plot.set_xticks(())\n",
        "    l1_plot.set_yticks(())\n",
        "    l1_plot.set_xlabel(\"Class %i\" % k)\n",
        "plt.suptitle(\"Classification vector for...\")\n",
        "\n",
        "run_time = time.time() - t0\n",
        "print(\"Example run in %.3f s\" % run_time)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_csv = pd.read_csv(\"dataset/X.csv\")\n",
        "y_csv = pd.read_csv(\"dataset/y.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib.axes._axes import Axes\n",
        "\n",
        "def visualize(X, y, row_count, col_count, offset = 0):\n",
        "    # scale = np.abs(X).max()\n",
        "    scale = 255 # in case we only pick some data and none of them reach the max value (255). \n",
        "    fig, axes = plt.subplots(row_count, col_count, figsize=(10, 10))\n",
        "    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
        "\n",
        "    for i in range(row_count * col_count):\n",
        "        ax: Axes = axes[i // col_count, i % col_count]\n",
        "        \n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_xlabel(str(i+offset)+\": \"+str(int(y[i+offset])))\n",
        "        \n",
        "        ax.imshow(\n",
        "            X[i+offset].reshape(28, 28),\n",
        "            interpolation=\"nearest\",\n",
        "            cmap=plt.cm.RdBu,\n",
        "            vmin=-scale,\n",
        "            vmax=scale,\n",
        "        )\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "row_count = 10\n",
        "col_count = 10\n",
        "\n",
        "X_data = X_csv.to_numpy()\n",
        "X_data = X_data[:, 1:]\n",
        "y_data_temp = y_csv.to_numpy()\n",
        "y_data = np.zeros(len(y_data_temp))\n",
        "for k in range(len(y_data_temp)):\n",
        "    y_data[k] = y_data_temp[k][1]\n",
        "\n",
        "print(X_data.shape)\n",
        "print(y_data.shape)\n",
        "\n",
        "visualize(X_data, y_data, row_count, col_count, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FFNN:\n",
        "    def __init__(self):\n",
        "        self.x = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "a_k = FFNN()\n",
        "print(a_k.x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## With Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "# X, y = make_classification(n_samples=100, random_state=1)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\n",
        "clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
        "clf.predict_proba(X_test[:1])\n",
        "print(clf.score(X_test, y_test))\n",
        "# X_MLP = X_test[0:5]\n",
        "# y_MLP = clf.predict(X_MLP)\n",
        "# visualize(X_MLP, y_MLP, 1, 3)\n",
        "# print(X_MLP)\n",
        "# print(y_MLP)\n",
        "\n",
        "\n",
        "y_MLP = clf.predict(X_data)\n",
        "print(X_data.shape)\n",
        "print(y_MLP.shape)\n",
        "visualize(X_data, y_MLP, 10, 10, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "differences = 0\n",
        "for k in range(len(y_data)):\n",
        "    if int(y_data[k]) != int(y_MLP[k]):\n",
        "        differences += 1\n",
        "print(differences)\n",
        "\n",
        "print(str((1-differences/len(y_data)) * 100) + \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Literal, Callable, Union\n",
        "import numpy as np\n",
        "from numpy.typing import NDArray, ArrayLike\n",
        "\n",
        "\n",
        "class FFNNClassifier:\n",
        "    def __init__(self, \n",
        "            hidden_layer_sizes: NDArray,\n",
        "            activation_func: Literal['linear', 'relu', 'sigmoid', 'tanh', 'softmax'], \n",
        "            learning_rate: float,\n",
        "            verbose: int, # 0: no print, 1: print epoch progress\n",
        "            max_epoch: int,\n",
        "            batch_size: int,\n",
        "            loss_func: Literal['mean_squared_error', 'binary_cross_entropy', 'categorical_cross_entropy']\n",
        "        ):\n",
        "        self.hidden_layer_sizes = hidden_layer_sizes\n",
        "        self.X: NDArray = []\n",
        "        self.y: list[ArrayLike] = []\n",
        "        self.weights_history: list[NDArray] = [] # array of weight matrix. index is current epoch\n",
        "        self.biases_history: list[ArrayLike] = [] # array of bias list. index is current epoch\n",
        "        self.weight_gradients_history: list[NDArray] = [] # array of weight gradients. index is current epoch\n",
        "\n",
        "        self.activation_func = activation_func\n",
        "        self.learning_rate = learning_rate\n",
        "        self.verbose = verbose\n",
        "        self.epoch_amount = max_epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.loss_func = loss_func\n",
        "\n",
        "    # return [ matrix, matrix, matrix ... ] where matrix is the weight adjacency matrix for each layer. length should be number of layers - 1 because its like the edges/connection between the nodes\n",
        "    def _generate_initial_weights(self):\n",
        "        \n",
        "        # delete this\n",
        "        # weights, nodes, nodes_active, biases = self._generate_new_empty_layers()\n",
        "        # return weights\n",
        "        return [np.array([[0.15, 0.25], [0.2, 0.3]]), np.array([[0.4, 0.5], [0.45, 0.55]])]\n",
        "    \n",
        "    # return [ float, float, float ... ] where float is the bias for each layer. length should be number of layers - 1 because input layer does not have bias\n",
        "    def _generate_initial_biases(self):\n",
        "        \n",
        "        # delete this\n",
        "        # weights, nodes, nodes_active, biases = self._generate_new_empty_layers()\n",
        "        # return biases\n",
        "        return [np.array([0.35, 0.35]), np.array([0.6, 0.6])]\n",
        "        \n",
        "\n",
        "# region functions\n",
        "    def _activation_function(self, x: Union[float, NDArray], func: str):\n",
        "        if func == 'linear': return x\n",
        "        elif func == 'relu': return np.maximum(0, x)\n",
        "        elif func == 'sigmoid': return 1.0/(1.0 + np.exp(-x))\n",
        "        elif func == 'tanh': return np.tanh(x)\n",
        "        elif func == 'softmax':\n",
        "            exp_x = np.exp(x - np.max(x, axis=1, keepdims=True)) \n",
        "            return exp_x / np.sum(exp_x, axis=1, keepdims=True) # keepdims=True will keep the dimension of the original array\n",
        "        raise \"Activation function not supported!\"\n",
        "    \n",
        "    def _activation_derived_function(self, x: Union[float, NDArray], func: str):\n",
        "        if func == 'linear': return np.ones_like(x)\n",
        "        elif func == 'relu': return np.where(x > 0, 1, 0)\n",
        "        elif func == 'sigmoid':\n",
        "            sig = self._activation_function(x, 'sigmoid')\n",
        "            return sig * (1 - sig)\n",
        "        elif func == 'tanh': \n",
        "            p = 2.0/(np.exp(x) - np.exp(-x))\n",
        "            return p*p\n",
        "        elif func == 'softmax': # TODO: Check if this implementation is correct\n",
        "            delta = np.zeros_like(x) \n",
        "            for i in range(x.shape[0]): delta[i, i] = 1\n",
        "            softmax = self._activation_function(x, 'softmax')\n",
        "            return softmax*(delta-softmax) \n",
        "        raise \"Activation function not supported!\"\n",
        "    \n",
        "\n",
        "    def _loss_function(self, y_act, y_pred, number_of_classes, func: str):\n",
        "        if func == 'mean_squared_error': return FFNNClassifier.mean_squared_error(y_act, y_pred)\n",
        "        elif func == 'binary_cross_entropy': return FFNNClassifier.binary_cross_entropy(y_act, y_pred)\n",
        "        elif func == 'categorical_cross_entropy': return FFNNClassifier.categorical_cross_entropy(y_act, y_pred, number_of_classes)\n",
        "    def mean_squared_error(y_act, y_pred):\n",
        "        return 2/len(y_act) * (y_act - y_pred)\n",
        "        res = 0\n",
        "        n = len(y_act)\n",
        "        for i in range(n):\n",
        "            minus = y_act - y_pred\n",
        "            res += minus*minus\n",
        "        return res/n\n",
        "    def binary_cross_entropy(y_act, y_pred, n):\n",
        "        res = 0\n",
        "        n = len(y_act)\n",
        "        for i in range(n):\n",
        "            res += y_act*np.log(y_pred) + (1-y_act)*np.log(1-y_pred)\n",
        "        return -res/n\n",
        "    def categorical_cross_entropy(y_act, y_pred, c):\n",
        "        res = 0\n",
        "        n = len(y_act)\n",
        "        for i in range(n):\n",
        "            for j in range(c):\n",
        "                res += y_act[i][j]*np.log(y_pred[i][j])\n",
        "        return -res/n\n",
        "\n",
        "# endregion functions\n",
        "\n",
        "\n",
        "# region getters setters\n",
        "    \n",
        "    # Can only be called after setting X and y\n",
        "    def _get_hidden_layer_sizes(self) -> np.typing.NDArray:\n",
        "        if len(self.X) == 0:\n",
        "            raise \"len(self.X) == 0\"\n",
        "        if len(self.X[0]) == 0:\n",
        "            raise \"len(self.X[0]) == 0\"\n",
        "        if len(self.y) == 0:\n",
        "            raise \"len(self.y) == 0\"\n",
        "        \n",
        "        len_features = len(self.X[0])\n",
        "        len_classes = self._get_number_of_classes()\n",
        "        layer_sizes = np.zeros(len(self.hidden_layer_sizes)+2, dtype=int)\n",
        "        layer_sizes[0] = len_features\n",
        "        for i in range(1, len(self.hidden_layer_sizes)+1):\n",
        "            layer_sizes[i] = self.hidden_layer_sizes[i-1]\n",
        "        layer_sizes[len(layer_sizes)-1] = len_classes\n",
        "        \n",
        "        return layer_sizes\n",
        "    \n",
        "    # Can only be called after setting X and y\n",
        "    def _generate_new_empty_layers(self):\n",
        "        if len(self.X) == 0:\n",
        "            raise \"len(self.X) == 0\"\n",
        "        if len(self.X[0]) == 0:\n",
        "            raise \"len(self.X[0]) == 0\"\n",
        "        if len(self.y) == 0:\n",
        "            raise \"len(self.y) == 0\"\n",
        "        \n",
        "        layer_sizes = self._get_hidden_layer_sizes()\n",
        "        network_depth = len(layer_sizes)\n",
        "        weights = []\n",
        "        biases = []\n",
        "        nodes = []\n",
        "        nodes_active = []\n",
        "        for i in range(network_depth-1):\n",
        "            weights.append(np.zeros((layer_sizes[i], layer_sizes[i+1])))\n",
        "            biases.append(np.zeros(layer_sizes[i+1]))\n",
        "        for i in range(network_depth):\n",
        "            nodes.append(np.zeros(layer_sizes[i]))\n",
        "            nodes_active.append(np.zeros(layer_sizes[i]))\n",
        "        \n",
        "        return weights, nodes, nodes_active, biases\n",
        "\n",
        "\n",
        "    # Can only be called after setting X and y\n",
        "    def _get_number_of_classes(self):\n",
        "        return len(np.unique(self.y))\n",
        "    \n",
        "# endregion getters setters\n",
        "\n",
        "    def fit(self, X: NDArray, y: NDArray):\n",
        "        # clean up in case this function is called multiple times\n",
        "        self.X: NDArray = []\n",
        "        self.y: ArrayLike = []\n",
        "        self.weights_history: list[NDArray] = []\n",
        "        self.biases_history: list[ArrayLike] = []\n",
        "        self.weight_gradients_history: list[NDArray] = []\n",
        "\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        initial_weight = self._generate_initial_weights()\n",
        "        initial_bias = self._generate_initial_biases()\n",
        "        self.weights_history.append(initial_weight)\n",
        "        self.biases_history.append(initial_bias)\n",
        "        initial_gradients = [np.zeros_like(w) for w in initial_weight]\n",
        "        self.weight_gradients_history.append(initial_gradients)\n",
        "\n",
        "        layer_sizes = self._get_hidden_layer_sizes()\n",
        "        network_depth = len(layer_sizes)\n",
        "        number_of_classes = self._get_number_of_classes()\n",
        "\n",
        "        for epoch in range(self.epoch_amount):\n",
        "            # for current_dataset_idx in range(len(self.X)):\n",
        "            current_dataset_idx = 0\n",
        "            while current_dataset_idx < len(self.X):\n",
        "                weights, nodes, nodes_active, biases = self._generate_new_empty_layers() # will be filled with weights based on the previous epoch. Will be appended to the history after the end of the epoch\n",
        "\n",
        "                # Feed Forward\n",
        "                until_idx = min(current_dataset_idx+self.batch_size, len(self.X))\n",
        "                nodes[0] = self.X[current_dataset_idx:until_idx]\n",
        "                nodes_active[0] = self.X[current_dataset_idx:until_idx] # not passed to activation function for the first layer\n",
        "                \n",
        "                for k in range(1, network_depth):\n",
        "                    w_k = self.weights_history[-1][k-1].T\n",
        "                    b_k = self.biases_history[-1][k-1]\n",
        "                    h_k_min_1 = nodes_active[k-1]\n",
        "\n",
        "                    b_k_broadcasted = b_k[:, np.newaxis] # copy the bias for each row in the matrix\n",
        "\n",
        "                    a_k = b_k_broadcasted + np.dot(w_k, h_k_min_1.T)\n",
        "                    nodes[k] = a_k.T\n",
        "                    nodes_active[k] = self._activation_function(a_k, self.activation_func).T\n",
        "\n",
        "                loss_grad = self._loss_function(\n",
        "                    y_act=self.y, \n",
        "                    y_pred=nodes_active[network_depth-1], \n",
        "                    number_of_classes=number_of_classes, \n",
        "                    func=self.loss_func\n",
        "                )\n",
        "\n",
        "\n",
        "                # Backward Propagation\n",
        "                delta = loss_grad * self._activation_derived_function(nodes[-1], self.activation_func)\n",
        "                weight_gradiens = np.zeros_like(self.weights_history[-1])\n",
        "                bias_gradiens = np.zeros_like(self.biases_history[-1])\n",
        "\n",
        "                weight_gradiens[network_depth-2] = np.dot(nodes_active[-2].T, delta) # cant use dot because its 1D array and numpy cant seem to transpose it\n",
        "                bias_gradiens[network_depth-2] = delta\n",
        "                \n",
        "                for k in range(network_depth-2, 0, -1): # from the last hidden layer (not including the output layer)\n",
        "                    w = self.weights_history[-1][k]\n",
        "                    delta = np.dot(delta, w.T) * self._activation_derived_function(nodes[k], self.activation_func)\n",
        "                    weight_gradiens[network_depth-2-k] = np.outer(nodes_active[k-1].T, delta)\n",
        "                    bias_gradiens[network_depth-2-k] = delta\n",
        "                \n",
        "                self.weight_gradients_history.append(weight_gradiens)\n",
        "                \n",
        "                # Update\n",
        "                for k in range(network_depth-1):\n",
        "                    w_k = self.weights_history[-1][k]\n",
        "                    weights[k] = w_k + self.learning_rate * weight_gradiens[k]\n",
        "                    biases[k] = w_k - self.learning_rate * bias_gradiens[k]\n",
        "                self.weights_history.append(weights)\n",
        "                self.biases_history.append(biases)\n",
        "\n",
        "            \n",
        "                current_dataset_idx += self.batch_size\n",
        "            #### while loop ends here ##############################################\n",
        "\n",
        "\n",
        "            if self.verbose == 1:\n",
        "                print(f\"Epoch {epoch+1}/{self.epoch_amount} done\")\n",
        "            elif self.verbose == 2:\n",
        "                print(f\"========================================\")\n",
        "                print(f\"Epoch {epoch+1}/{self.epoch_amount} done\")\n",
        "                print(f\"weights: {self.weights_history[-1]}\")\n",
        "                print(f\"biases: {self.biases_history[-1]}\")\n",
        "\n",
        "\n",
        "    def predict(self, X_test: NDArray):\n",
        "        prediction = np.zeros(len(X_test))\n",
        "        for i in range(len(X_test)):\n",
        "            X = X_test[i]\n",
        "            len_layer = len(self.weights_history[-1])\n",
        "            for k in range(len_layer):\n",
        "                w_k = self.weights_history[-1][k]\n",
        "                b_k = self.biases_history[-1][k]\n",
        "\n",
        "                a_k = b_k + np.dot(w_k, X)\n",
        "                X = self._activation_function(a_k, self.activation_func)\n",
        "            \n",
        "            predicted_class = np.argmax(X) # idx with highest value. idx is also the class\n",
        "            prediction[i] = predicted_class\n",
        "            \n",
        "        return prediction\n",
        "\n",
        "    \n",
        "    def predict_proba(X_test: NDArray):\n",
        "        pass\n",
        "\n",
        "    def score(X_test, y_test):\n",
        "        pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nodes_active[0].shape: (1, 2)\n",
            "nodes_active[0]: [[0.05 0.1 ]]\n",
            "nodes_active[-1] [[0.75136507 0.77292847]]\n",
            "nodes[-1] [[1.10590597 1.2249214 ]]\n",
            "nodes[-1].T [[1.10590597]\n",
            " [1.2249214 ]]\n",
            "delta [[-0.13849856  0.03809824]]\n",
            "network_depth-2: 1\n",
            "bias_gradiens.shape: (2, 2)\n",
            "bias_gradiens[0].shape: (2,)\n",
            "bias_gradiens[1].shape: (2,)\n",
            "bias_gradiens[network_depth-2].shape: (2,)\n",
            "bias_gradiens[network_depth-2]: [0. 0.]\n",
            "Epoch 1/1 done\n",
            "Final Weights: [array([[0.14978072, 0.24975114],\n",
            "       [0.19956143, 0.29950229]]), array([[0.35891648, 0.51130127],\n",
            "       [0.40866619, 0.56137012]])]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ffnn = FFNNClassifier(\n",
        "    hidden_layer_sizes=[2],\n",
        "    activation_func=\"sigmoid\",\n",
        "    learning_rate=0.5,\n",
        "    verbose=1,\n",
        "    max_epoch=1,\n",
        "    batch_size=1,\n",
        "    loss_func=\"mean_squared_error\"\n",
        ")\n",
        "\n",
        "X_temp = np.array([[0.05, 0.1]])\n",
        "y_temp = np.array([0.01, 0.99])\n",
        "ffnn.fit(X_temp, y_temp)\n",
        "prediction = ffnn.predict(X_temp)\n",
        "# print(prediction)\n",
        "\n",
        "print(\"Final Weights:\", ffnn.weights_history[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ffnn = FFNNClassifier(\n",
        "    hidden_layer_sizes=(64, 64),\n",
        "    activation_func=\"relu\",\n",
        "    learning_rate=0.5,\n",
        "    max_epoch=20,\n",
        "    verbose=0,\n",
        "    max_epoch=5,\n",
        "    batch_size=10\n",
        ")\n",
        "ffnn.fit(X_train, y_train)\n",
        "prediction = ffnn.predict(X_test)\n",
        "visualize(X_test, prediction, 10, 10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from scipy import stats\n",
        "\n",
        "class NeuralNetworkVisualizer:\n",
        "    def __init__(self, layers, weights=None, gradients=None, biases=None):\n",
        "        self.layers = layers\n",
        "\n",
        "        if weights is not None:\n",
        "            self.weights = [w[0] if isinstance(w, list) and isinstance(w[0], np.ndarray) else w for w in weights]\n",
        "        else:\n",
        "            self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers) - 1)]\n",
        "\n",
        "        if gradients is not None:\n",
        "            self.gradients = [g[0] if isinstance(g, list) and isinstance(g[0], np.ndarray) else g for g in gradients]\n",
        "        else:\n",
        "            self.gradients = [np.random.randn(*w.shape) * 0.1 for w in self.weights]\n",
        "            \n",
        "        if biases is not None:\n",
        "            self.biases = [b[0] if isinstance(b, list) and isinstance(b[0], np.ndarray) else b for b in biases]\n",
        "        else:\n",
        "            self.biases = [np.random.randn(layers[i+1]) for i in range(len(layers) - 1)]\n",
        "\n",
        "    def plot_network(self):\n",
        "        G = nx.DiGraph()\n",
        "        positions = {}\n",
        "        node_count = 0\n",
        "        layer_spacing = 3\n",
        "        node_spacing = 1\n",
        "        colors = []\n",
        "        node_labels = {}\n",
        "\n",
        "        for layer_idx, num_nodes in enumerate(self.layers):\n",
        "            layer_start_node = node_count\n",
        "            for node_idx in range(num_nodes):\n",
        "                node_id = node_count\n",
        "                positions[node_id] = (layer_idx * layer_spacing, -node_idx * node_spacing)\n",
        "                G.add_node(node_id)\n",
        "                node_labels[node_id] = f\"{layer_idx},{node_idx}\"\n",
        "                if layer_idx == 0:\n",
        "                    colors.append(\"red\")  # Input layer\n",
        "                elif layer_idx == len(self.layers) - 1:\n",
        "                    colors.append(\"green\")  # Output layer\n",
        "                else:\n",
        "                    colors.append(\"blue\")  # Hidden layer\n",
        "                node_count += 1\n",
        "\n",
        "        edge_weights = {}\n",
        "        node_count = 0\n",
        "        \n",
        "        for layer_idx in range(len(self.layers) - 1):\n",
        "            layer_size = self.layers[layer_idx]\n",
        "            next_layer_size = self.layers[layer_idx + 1]\n",
        "            next_layer_start = node_count + layer_size\n",
        "            \n",
        "            for src in range(node_count, node_count + layer_size):\n",
        "                for dst_idx, dst in enumerate(range(next_layer_start, next_layer_start + next_layer_size)):\n",
        "                    weight = self.weights[layer_idx][src - node_count, dst_idx]\n",
        "                    G.add_edge(src, dst, weight=weight)\n",
        "                    edge_weights[(src, dst)] = f'{weight:.2f}'\n",
        "            \n",
        "            node_count += layer_size\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        \n",
        "        nx.draw(G, pos=positions, with_labels=False, node_size=500, node_color=colors, edge_color=\"gray\")\n",
        "        \n",
        "        edge_labels_pos = {}\n",
        "        for (u, v), label in edge_weights.items():\n",
        "            u_x, u_y = positions[u]\n",
        "            v_x, v_y = positions[v]\n",
        "            \n",
        "            mid_x = (u_x + v_x) / 2\n",
        "            mid_y = (u_y + v_y) / 2\n",
        "            \n",
        "            edge_hash = hash((u, v)) % 10\n",
        "            offset_x = 0.1 * (edge_hash % 3 - 1)\n",
        "            offset_y = 0.1 * ((edge_hash // 3) % 3 - 1)\n",
        "            \n",
        "            edge_labels_pos[(u, v)] = (mid_x + offset_x, mid_y + offset_y)\n",
        "        \n",
        "        for (u, v), label in edge_weights.items():\n",
        "            x, y = edge_labels_pos[(u, v)]\n",
        "            plt.text(x, y, label, fontsize=8, ha='center', va='center',\n",
        "                    bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
        "        \n",
        "        for layer_idx in range(len(self.layers)):\n",
        "            layer_name = \"Input Layer\" if layer_idx == 0 else \"Output Layer\" if layer_idx == len(self.layers) - 1 else f\"Hidden Layer {layer_idx}\"\n",
        "            plt.text(layer_idx * layer_spacing, 0.5, layer_name, fontsize=12, ha='center', bbox=dict(facecolor='white', alpha=0.5))\n",
        "            plt.text(layer_idx * layer_spacing, 0.25, f\"Layer {layer_idx}\", fontsize=10, ha='center', bbox=dict(facecolor='white', alpha=0.5))\n",
        "        \n",
        "        print(\"STRUKTUR JARINGAN\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_weight_distribution(self, layers_to_plot, plot_type='histogram'):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        if plot_type == 'histogram':\n",
        "            for layer_idx in layers_to_plot:\n",
        "                plt.hist(self.weights[layer_idx].flatten(), bins=20, alpha=0.5, label=f'Layer {layer_idx}')\n",
        "                \n",
        "        elif plot_type == 'line':\n",
        "            for layer_idx in layers_to_plot:\n",
        "                weights_flat = self.weights[layer_idx].flatten()\n",
        "                \n",
        "                density = stats.gaussian_kde(weights_flat)\n",
        "                \n",
        "                x_min, x_max = min(weights_flat), max(weights_flat)\n",
        "                x = np.linspace(x_min, x_max, 200)\n",
        "\n",
        "                plt.plot(x, density(x), label=f'Layer {layer_idx}')\n",
        "\n",
        "                plt.plot(weights_flat, np.zeros_like(weights_flat), '|', \n",
        "                        color=plt.gca().lines[-1].get_color(), alpha=0.3, markersize=5)\n",
        "        \n",
        "        plt.title(\"Weight Distribution\")\n",
        "        plt.xlabel(\"Weight Value\")\n",
        "        plt.ylabel(\"Density\" if plot_type == 'line' else \"Frequency\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_gradient_distribution(self, layers_to_plot, plot_type='histogram'):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        if plot_type == 'histogram':\n",
        "            for layer_idx in layers_to_plot:\n",
        "                plt.hist(self.gradients[layer_idx].flatten(), bins=20, alpha=0.5, label=f'Layer {layer_idx}')\n",
        "                \n",
        "        elif plot_type == 'line':\n",
        "            for layer_idx in layers_to_plot:\n",
        "                gradients_flat = self.gradients[layer_idx].flatten()\n",
        "\n",
        "                density = stats.gaussian_kde(gradients_flat)\n",
        "\n",
        "                x_min, x_max = min(gradients_flat), max(gradients_flat)\n",
        "                x = np.linspace(x_min, x_max, 200)\n",
        "\n",
        "                plt.plot(x, density(x), label=f'Layer {layer_idx}')\n",
        "\n",
        "                plt.plot(gradients_flat, np.zeros_like(gradients_flat), '|', \n",
        "                            color=plt.gca().lines[-1].get_color(), alpha=0.3, markersize=5)\n",
        "        \n",
        "        plt.title(\"Gradient Distribution\")\n",
        "        plt.xlabel(\"Gradient Value\")\n",
        "        plt.ylabel(\"Density\" if plot_type == 'line' else \"Frequency\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "layers = [3, 4, 2]\n",
        "\n",
        "custom_weights = [\n",
        "    np.random.normal(0, 0.5, (3, 4)),\n",
        "    np.random.normal(0, 0.5, (4, 2))\n",
        "]\n",
        "\n",
        "custom_gradients = [\n",
        "    np.random.normal(0, 0.1, (3, 4)),\n",
        "    np.random.normal(0, 0.1, (4, 2))\n",
        "]\n",
        "\n",
        "custom_biases = [\n",
        "    np.random.normal(0, 0.3),  # Biases for the first hidden layer\n",
        "    np.random.normal(0, 0.3)   # Biases for the output layer\n",
        "]\n",
        "\n",
        "visualizer = NeuralNetworkVisualizer(layers, \n",
        "                                   weights=custom_weights,\n",
        "                                   gradients=custom_gradients)\n",
        "\n",
        "visualizer.plot_network()\n",
        "\n",
        "visualizer.plot_weight_distribution([0, 1], plot_type='line')\n",
        "\n",
        "visualizer.plot_gradient_distribution([0, 1], plot_type='line')\n",
        "\n",
        "visualizer.plot_weight_distribution([0, 1], plot_type='histogram')\n",
        "\n",
        "visualizer.plot_weight_distribution([0])  # Plot only first layer weights\n",
        "visualizer.plot_gradient_distribution([1])  # Plot only output layer gradients"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
